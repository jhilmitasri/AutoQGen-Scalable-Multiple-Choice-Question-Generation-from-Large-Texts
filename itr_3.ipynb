{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e31e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and API Key Configuration\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local environment (make sure you have OPENAI_API_KEY set):\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Initialize the client.\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    print(\"OpenAI client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    print(\"Please ensure your API key is configured correctly.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to point to the PDF you want to process\n",
    "PDF_FILE_PATH = \"Documents/The 100 Page Machine Learning Book Part2.pdf\"\n",
    "OUTPUT_JSON_PATH = \"quiz_output.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424c7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Document Processing and Parsing Functions\n",
    "\n",
    "import ftfy\n",
    "\n",
    "def clean_and_normalize_text(text: str) -> str:\n",
    "    \"\"\"Fixes Unicode errors and normalizes text.\"\"\"\n",
    "    # ftfy.fix_text handles the weird characters like '\\ufb01'\n",
    "    # The encode/decode step handles any remaining escape sequences.\n",
    "    return ftfy.fix_text(text)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text content from a PDF file.\"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "        \n",
    "        cleaned_text = clean_and_normalize_text(full_text)\n",
    "\n",
    "        print(f\"✅ Successfully extracted {len(cleaned_text.split())} words from {pdf_path}.\")\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading PDF {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 200) -> list[str]:\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunks.append(\" \".join(words[i:i + chunk_size]))\n",
    "    print(f\"Text split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def parse_json_from_response(response_text: str) -> dict | None:\n",
    "    \"\"\"Safely extracts a JSON object from a string, even with surrounding text.\"\"\"\n",
    "    # Use regex to find the JSON block, which handles leading/trailing text\n",
    "    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    if not json_match:\n",
    "        print(\"Parser Error: No JSON object found in the response.\")\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(json_match.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Parser Error: Failed to decode JSON from the extracted string.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b08e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Core LLM Functions\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained model once at the beginning of the script\n",
    "# This model is small, fast, and effective\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def retrieve_relevant_passages_semantic(concept: str, text_chunks: list[str], chunk_embeddings, top_k: int = 2) -> list[str]:\n",
    "    \"\"\"Retrieves the most relevant text chunks using semantic search.\"\"\"\n",
    "    # 1. Create an embedding for the concept (the query)\n",
    "    concept_embedding = embedding_model.encode(concept, convert_to_tensor=True)\n",
    "    \n",
    "    # 2. Calculate cosine similarity between the concept and all text chunks\n",
    "    cosine_scores = util.cos_sim(concept_embedding, chunk_embeddings)[0]\n",
    "    \n",
    "    # 3. Get the indices of the top_k most similar chunks\n",
    "    top_results_indices = np.argsort(-cosine_scores.cpu())[:top_k]\n",
    "    \n",
    "    # 4. Return the actual text chunks\n",
    "    return [text_chunks[i] for i in top_results_indices]\n",
    "\n",
    "def extract_concepts_from_chunk(chunk: str, client) -> list[str]:\n",
    "    \"\"\"Uses gpt-3.5-turbo to extract key concepts from a text chunk.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following text excerpt, please extract the most critical main ideas and concepts.\n",
    "    Respond with a simple bulleted list.\n",
    "\n",
    "    Excerpt:\n",
    "    \\\"\\\"\\\"\n",
    "    {chunk}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    Concepts:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return [line.strip('- ') for line in content.strip().split('\\n') if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"API Error during concept extraction: {e}\")\n",
    "        return []\n",
    "\n",
    "def synthesize_concepts(all_concepts: list[str], client) -> list[str]:\n",
    "    \"\"\"Merges and deduplicates a list of concepts using gpt-3.5-turbo.\"\"\"\n",
    "    concepts_str = \"\\n\".join([f\"- {c}\" for c in all_concepts])\n",
    "    prompt = f\"\"\"\n",
    "                You are given a long, potentially repetitive list of concepts extracted from a document.\n",
    "                Your task is to synthesize this into a final, consolidated list of unique concepts.\n",
    "                Carefully merge concepts that refer to the same idea (e.g., \"Linear Regression Model\" and \"Linear Regression Algorithm\").\n",
    "                Eliminate any concepts that are too generic or not technical.\n",
    "\n",
    "                Extracted Concepts:\n",
    "                \\\"\\\"\\\"\n",
    "                {concepts_str}\n",
    "                \\\"\\\"\\\"\n",
    "\n",
    "                Return ONLY a single, clean, bulleted list of the final consolidated concepts.\n",
    "            \"\"\" \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return [line.strip('- ') for line in content.strip().split('\\n') if line.strip()]\n",
    "\n",
    "def retrieve_relevant_passages(concept: str, text_chunks: list[str], top_k: int = 2) -> list[str]:\n",
    "    \"\"\"Retrieves the most relevant text chunks for a given concept using keyword matching.\"\"\"\n",
    "    concept_words = set(concept.lower().split())\n",
    "    scored_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        score = len(concept_words.intersection(chunk_words))\n",
    "        if score > 0:\n",
    "            scored_chunks.append((score, chunk))\n",
    "\n",
    "    scored_chunks.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [chunk for score, chunk in scored_chunks[:top_k]]\n",
    "\n",
    "def generate_question_with_difficulty(concept: str, passages: list[str], difficulty: str, client) -> dict | None:\n",
    "    \"\"\"Generates a question with a specific difficulty level (CORRECTED VERSION).\"\"\"\n",
    "\n",
    "    difficulty_instructions = {\n",
    "        \"easy\": \"The question should test basic recall or understanding of a key definition from the text (Bloom's Taxonomy: Remembering/Understanding).\",\n",
    "        \"medium\": \"The question should require applying a concept to a new context or analyzing the relationship between ideas from the text (Bloom's Taxonomy: Applying/Analyzing).\",\n",
    "        \"hard\": \"The question should require evaluating the strengths/weaknesses of an argument or synthesizing information from multiple passages to form a conclusion (Bloom's Taxonomy: Evaluating/Creating).\"\n",
    "    }\n",
    "    \n",
    "    instruction = difficulty_instructions.get(difficulty, difficulty_instructions['medium'])\n",
    "    passages_str = \"\\\\n\\\\n---\\\\n\\\\n\".join(passages)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            Act as an expert educator. Your task is to create one high-quality, multiple-choice question based on the provided main idea and source text.\n",
    "\n",
    "            **Main Idea**: \"{concept}\"\n",
    "            **Source Text**:\n",
    "            \\\"\\\"\\\"\n",
    "            {passages_str}\n",
    "            \\\"\\\"\\\"\n",
    "\n",
    "            **Difficulty Instruction**: {instruction}\n",
    "\n",
    "            Follow these steps:\n",
    "            1.  **Reasoning Step**: First, silently think step-by-step. What is the core conceptual knowledge being tested? What would be common misconceptions for the distractors? How can I phrase the question to test for deep understanding rather than simple recall?\n",
    "            2.  **Generation Step**: Based on your reasoning, generate a single JSON object with the exact keys: \"question\", \"choices\", \"correct_answer\", \"explanation\".\n",
    "\n",
    "            **CRITICAL INSTRUCTIONS**:\n",
    "            -   The \"choices\" must be a list of 4 strings.\n",
    "            -   The value for \"correct_answer\" MUST be the full text of the correct option, NOT the letter (e.g., \"A\" or \"B\").\n",
    "            -   All choices, including distractors, must be plausible and directly related to the main idea.\n",
    "            -   The explanation must be concise (2-3 sentences) and clearly state why the correct answer is right based on the source text.\n",
    "\n",
    "            Your response MUST be ONLY the single JSON object.\n",
    "\n",
    "            Example of a perfect response:\n",
    "            {{\n",
    "                \"question\": \"What is the primary trade-off controlled by the hyperparameter C in a soft-margin SVM?\",\n",
    "                \"choices\": [\n",
    "                    \"Margin size vs. model complexity\", \n",
    "                    \"Margin size vs. classification accuracy on training data\", \n",
    "                    \"Kernel type vs. training speed\", \n",
    "                    \"Number of support vectors vs. feature dimensions\"\n",
    "                ],\n",
    "                \"correct_answer\": \"Margin size vs. classification accuracy on training data\",\n",
    "                \"explanation\": \"The hyperparameter C controls the trade-off between maximizing the margin for better generalization and minimizing the misclassification of training examples for a better fit to the data.\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # 1. Make the API call\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # 2. DEFINE response_content from the result\n",
    "        response_content = response.choices[0].message.content\n",
    "        \n",
    "        # 3. NOW use response_content to parse the JSON\n",
    "        parsed_json = parse_json_from_response(response_content)\n",
    "        \n",
    "        if parsed_json:\n",
    "            parsed_json['difficulty'] = difficulty\n",
    "        \n",
    "        return parsed_json\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed for concept '{concept}': {e}\")\n",
    "        return None\n",
    "\n",
    "def score_question_quality(question_data: dict, client) -> int:\n",
    "    \"\"\"Uses an AI judge to score the quality of a generated question (1-5).\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please evaluate the quality of the following multiple-choice question on a scale of 1 to 5,\n",
    "    where 1 is poor and 5 is excellent. Consider its clarity, conceptual depth, and the plausibility of its distractors.\n",
    "\n",
    "    Question: {question_data['question']}\n",
    "    Choices: {question_data['choices']}\n",
    "\n",
    "    Return ONLY a single integer score between 1 and 5.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", # The cheap model is fine for this simple task\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return int(response.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Quality scoring failed: {e}\")\n",
    "        return 0 # Return a default low score on failure\n",
    "\n",
    "def pre_filter_question(question_data: dict) -> bool:\n",
    "    \"\"\"Performs simple, non-LLM checks on a generated question (Robust Version).\"\"\"\n",
    "    if not question_data:\n",
    "        return False\n",
    "\n",
    "    choices_data = question_data.get(\"choices\", [])\n",
    "    \n",
    "    # --- NEW LOGIC to handle both lists and dictionaries ---\n",
    "    actual_choices = []\n",
    "    if isinstance(choices_data, dict):\n",
    "        # If it's a dictionary, iterate over its values\n",
    "        actual_choices = list(choices_data.values())\n",
    "    elif isinstance(choices_data, list):\n",
    "        # If it's a list, use it directly\n",
    "        actual_choices = choices_data\n",
    "    \n",
    "    if not actual_choices:\n",
    "        return False\n",
    "    # --- END OF NEW LOGIC ---\n",
    "\n",
    "    # Now, perform the checks on the actual choice text\n",
    "    if any(len(str(c).split()) < 2 for c in actual_choices):\n",
    "        print(\"  -> Pre-filter fail: A choice was too short.\")\n",
    "        return False\n",
    "        \n",
    "    if len(question_data.get(\"explanation\", \"\").split()) < 5:\n",
    "        print(\"  -> Pre-filter fail: Explanation was too short.\")\n",
    "        return False\n",
    "        \n",
    "    if \"placeholder\" in question_data.get(\"question\", \"\").lower():\n",
    "        print(\"  -> Pre-filter fail: Question contained placeholder text.\")\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def extract_concepts_from_batch(combined_prompt: str, client) -> list[str]:\n",
    "    \"\"\"\n",
    "    Sends a batch of chunks in a single prompt to the LLM and parses the\n",
    "    structured response to extract a flat list of concepts (CORRECTED).\n",
    "    \"\"\"\n",
    "    full_prompt = combined_prompt + \"\\n\\nReturn a bulleted list of concepts for each excerpt under its corresponding heading.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # --- DEBUGGING LINE ---\n",
    "        # print(\"--- Raw LLM Output for Batch ---\\n\", content, \"\\n---------------------------------\")\n",
    "        \n",
    "        extracted_concepts = []\n",
    "        \n",
    "        # Split by the correct newline character '\\n'\n",
    "        lines = content.strip().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line.startswith('-'):\n",
    "                concept = cleaned_line.lstrip('- ').strip()\n",
    "                if concept:\n",
    "                    extracted_concepts.append(concept)\n",
    "                    \n",
    "        return extracted_concepts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API Error during batch concept extraction: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def verify_question_semantic(question_data: dict, embedding_model, threshold=0.40) -> bool:\n",
    "    \"\"\"Verifies a question by checking the semantic similarity of the answer to its source (Robust Version).\"\"\"\n",
    "    \n",
    "    answer_text = question_data['correct_answer']\n",
    "    choices = question_data['choices']\n",
    "\n",
    "    # --- NEW LOGIC TO HANDLE INCONSISTENT FORMATS ---\n",
    "    # Check if the answer is a letter (e.g., \"A\", \"B\") and choices is a dict\n",
    "    if isinstance(choices, dict) and answer_text.upper() in choices:\n",
    "        answer_text = choices[answer_text.upper()]\n",
    "    # Check if the answer is a letter and choices is a list (less common)\n",
    "    elif isinstance(choices, list) and answer_text.upper() in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        # Simple mapping for A=0, B=1, etc.\n",
    "        idx = ord(answer_text.upper()) - ord('A')\n",
    "        if idx < len(choices):\n",
    "            answer_text = choices[idx]\n",
    "    # --- END OF NEW LOGIC ---\n",
    "\n",
    "    source_context = \" \".join(question_data['source_passages'])\n",
    "\n",
    "    answer_embedding = embedding_model.encode(answer_text, convert_to_tensor=True)\n",
    "    context_embedding = embedding_model.encode(source_context, convert_to_tensor=True)\n",
    "\n",
    "    similarity_score = util.cos_sim(answer_embedding, context_embedding)[0][0]\n",
    "\n",
    "    print(f\"  -> Verification Score: {similarity_score:.4f} (Threshold: {threshold})\")\n",
    "\n",
    "    return similarity_score >= threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text chunks and embeddings from cache file: The 100 Page Machine Learning Book Part2.pdf.pkl\n",
      "\n",
      "--- Starting Stage 1: Concept Extraction (in Batches) ---\n",
      "Extracting concepts from chunks 1-4...\n",
      "\n",
      "Extracted 76 raw concepts. Now synthesizing...\n",
      "Synthesized down to 53 final concepts.\n",
      "\n",
      "--- Starting Stage 2: Question Generation ---\n",
      "Processing concept 1/53 (Difficulty: easy): 'Derivative...'\n",
      "  -> Verification Score: 0.5359 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 2/53 (Difficulty: medium): 'Gradient...'\n",
      "  -> Verification Score: 0.2772 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 3/53 (Difficulty: hard): 'Function...'\n",
      "  -> Verification Score: -0.0126 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 4/53 (Difficulty: easy): 'Constant value...'\n",
      "  -> Pre-filter fail: A choice was too short.\n",
      "  -> ❌ Discarded question that failed pre-filter.\n",
      "Processing concept 5/53 (Difficulty: medium): 'Domain...'\n",
      "  -> Verification Score: 0.2438 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 6/53 (Difficulty: hard): 'Differentiation...'\n",
      "  -> Verification Score: 0.0824 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 7/53 (Difficulty: easy): 'Chain rule...'\n",
      "  -> Pre-filter fail: A choice was too short.\n",
      "  -> ❌ Discarded question that failed pre-filter.\n",
      "Processing concept 8/53 (Difficulty: medium): 'Basic functions...'\n",
      "  -> Verification Score: 0.1273 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 9/53 (Difficulty: hard): 'Partial derivatives...'\n",
      "  -> Verification Score: 0.0097 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 10/53 (Difficulty: easy): 'Vector...'\n",
      "  -> Verification Score: 0.0066 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 11/53 (Difficulty: medium): 'Expectation...'\n",
      "  -> Verification Score: 0.6306 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 5\n",
      "Processing concept 12/53 (Difficulty: hard): 'Standard deviation...'\n",
      "  -> Verification Score: 0.1559 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 13/53 (Difficulty: easy): 'Probability mass function...'\n",
      "  -> Verification Score: 0.0819 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 14/53 (Difficulty: medium): 'Probability density function...'\n",
      "  -> Verification Score: 0.2143 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 15/53 (Difficulty: hard): 'Random variable...'\n",
      "  -> Verification Score: 0.0907 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 16/53 (Difficulty: easy): 'Discrete random variable...'\n",
      "  -> Verification Score: 0.0840 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 17/53 (Difficulty: medium): 'Continuous random variable...'\n",
      "  -> Verification Score: 0.1405 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 18/53 (Difficulty: hard): 'Parameter estimation...'\n",
      "  -> Verification Score: 0.6266 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 5\n",
      "Processing concept 19/53 (Difficulty: easy): 'Bayes' Rule...'\n",
      "  -> Verification Score: 0.2368 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 20/53 (Difficulty: medium): 'Classiﬁcation...'\n",
      "  -> Verification Score: 0.2380 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 21/53 (Difficulty: hard): 'Regression...'\n",
      "  -> Verification Score: 0.3757 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 22/53 (Difficulty: easy): 'Model-Based Learning...'\n",
      "  -> Verification Score: 0.4533 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 23/53 (Difficulty: medium): 'Instance-Based Learning...'\n",
      "  -> Verification Score: 0.4407 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 24/53 (Difficulty: hard): 'Shallow Learning...'\n",
      "  -> Verification Score: 0.3240 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 25/53 (Difficulty: easy): 'Deep Learning...'\n",
      "  -> Verification Score: 0.3370 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 26/53 (Difficulty: medium): 'Binary classification...'\n",
      "  -> Verification Score: 0.3852 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 27/53 (Difficulty: hard): 'Multiclass classification...'\n",
      "  -> Verification Score: 0.4076 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 28/53 (Difficulty: easy): 'Model-based learning algorithm...'\n",
      "  -> Verification Score: 0.4541 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 29/53 (Difficulty: medium): 'Instance-based learning algorithm...'\n",
      "  -> Verification Score: 0.4359 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 30/53 (Difficulty: hard): 'Unbiased estimators...'\n",
      "  -> Verification Score: 0.4175 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 31/53 (Difficulty: easy): 'Gaussian function...'\n",
      "  -> Verification Score: 0.4338 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 32/53 (Difficulty: medium): 'Prior...'\n",
      "  -> Verification Score: 0.4416 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 5\n",
      "Processing concept 33/53 (Difficulty: hard): 'Maximum-likelihood...'\n",
      "  -> Verification Score: 0.4608 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 34/53 (Difficulty: easy): 'Classiﬁcation vs. Regression...'\n",
      "  -> Verification Score: 0.3757 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 35/53 (Difficulty: medium): 'Decision tree...'\n",
      "  -> Verification Score: 0.1035 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 36/53 (Difficulty: hard): 'ID3 algorithm...'\n",
      "  -> Verification Score: 0.2756 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 37/53 (Difficulty: easy): 'Entropy...'\n",
      "  -> Verification Score: 0.2368 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 38/53 (Difficulty: medium): 'Split criterion...'\n",
      "  -> Verification Score: 0.4520 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 5\n",
      "Processing concept 39/53 (Difficulty: hard): 'Hyperparameters...'\n",
      "  -> Verification Score: 0.1760 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 40/53 (Difficulty: easy): 'Maximum depth...'\n",
      "  -> Verification Score: 0.1458 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 41/53 (Difficulty: medium): 'Noise in data...'\n",
      "  -> Verification Score: 0.4076 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 42/53 (Difficulty: hard): 'Hinge loss function...'\n",
      "  -> Verification Score: 0.1975 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 43/53 (Difficulty: easy): 'Soft-margin SVM...'\n",
      "  -> Verification Score: 0.3725 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 44/53 (Difficulty: medium): 'Hard-margin SVM...'\n",
      "  -> Verification Score: 0.3947 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 45/53 (Difficulty: hard): 'Cost function...'\n",
      "  -> Verification Score: 0.2175 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 46/53 (Difficulty: easy): 'Tradeoff parameter C...'\n",
      "  -> Verification Score: 0.4400 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 47/53 (Difficulty: medium): 'Kernel trick...'\n",
      "  -> Verification Score: 0.1780 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 48/53 (Difficulty: hard): 'Transformation of data...'\n",
      "  -> Verification Score: 0.4618 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 49/53 (Difficulty: easy): 'Lagrange multipliers...'\n",
      "  -> Verification Score: 0.4601 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 50/53 (Difficulty: medium): 'Support Vector Machine (SVM)...'\n",
      "  -> Verification Score: 0.4113 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "Processing concept 51/53 (Difficulty: hard): 'Kernel functions...'\n",
      "  -> Verification Score: 0.0907 (Threshold: 0.4)\n",
      "  -> ❌ Discarded question that failed verification.\n",
      "Processing concept 52/53 (Difficulty: easy): 'Convex quadratic optimization...'\n",
      "  -> Pre-filter fail: A choice was too short.\n",
      "  -> ❌ Discarded question that failed pre-filter.\n",
      "Processing concept 53/53 (Difficulty: medium): 'Quadratic programming algorithms...'\n",
      "  -> Verification Score: 0.4357 (Threshold: 0.4)\n",
      "  -> ✅ Verified and Kept question with score: 4\n",
      "\n",
      "✅ Pipeline complete! Generated 19 questions.\n",
      "Output saved to quiz_output.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run the Full Pipeline (with Caching and Bug Fixes)\n",
    "import pickle\n",
    "\n",
    "def run_full_pipeline(text_chunks, chunk_embeddings, client, desired_difficulties=[\"easy\", \"medium\", \"hard\"]):\n",
    "    # Stage 1: Extract and Synthesize Concepts\n",
    "    print(\"\\n--- Starting Stage 1: Concept Extraction (in Batches) ---\")\n",
    "    all_chunk_concepts = []\n",
    "    batch_size = 5 # Process 5 chunks at a time\n",
    "    for i in range(0, len(text_chunks), batch_size):\n",
    "        batch_chunks = text_chunks[i:i + batch_size]\n",
    "        \n",
    "        combined_prompt = \"\"\"For each document excerpt below, act as a subject matter expert and extract the key concepts.\n",
    "                        A 'concept' should be a core technical term, algorithm, or principle that is essential for understanding the text.\n",
    "                        Ignore introductory phrases, examples, or simple definitions. Focus on the main nouns or noun phrases.\n",
    "\n",
    "                        \"\"\"\n",
    "\n",
    "        for j, chunk in enumerate(batch_chunks):\n",
    "            combined_prompt += f\"--- Excerpt {j+1} ---\\n{chunk}\\n\\n\"\n",
    "        \n",
    "        print(f\"Extracting concepts from chunks {i+1}-{min(i+batch_size, len(text_chunks))}...\")\n",
    "        all_chunk_concepts.extend(extract_concepts_from_batch(combined_prompt, client))\n",
    "\n",
    "    print(f\"\\nExtracted {len(all_chunk_concepts)} raw concepts. Now synthesizing...\")\n",
    "    final_concepts = synthesize_concepts(all_chunk_concepts, client)\n",
    "    print(f\"Synthesized down to {len(final_concepts)} final concepts.\")\n",
    "\n",
    "    # Stage 2: Retrieve and Generate Questions\n",
    "    print(\"\\n--- Starting Stage 2: Question Generation ---\")\n",
    "    generated_questions = []\n",
    "    \n",
    "    for i, concept in enumerate(final_concepts):\n",
    "        current_difficulty = desired_difficulties[i % len(desired_difficulties)]\n",
    "        \n",
    "        print(f\"Processing concept {i+1}/{len(final_concepts)} (Difficulty: {current_difficulty}): '{concept[:50]}...'\")\n",
    "        passages = retrieve_relevant_passages_semantic(concept, text_chunks, chunk_embeddings)\n",
    "        \n",
    "        if passages:\n",
    "            question_data = generate_question_with_difficulty(concept, passages, current_difficulty, client)\n",
    "\n",
    "            if pre_filter_question(question_data):\n",
    "                quality_score = score_question_quality(question_data, client)\n",
    "                question_data['quality_score'] = quality_score\n",
    "                \n",
    "                if quality_score >= 3:\n",
    "                    question_data['source_passages'] = passages\n",
    "                    is_verified = verify_question_semantic(question_data, embedding_model)\n",
    "                    if is_verified:\n",
    "                        generated_questions.append(question_data)\n",
    "                        print(f\"  -> ✅ Verified and Kept question with score: {quality_score}\")\n",
    "                    else:\n",
    "                        print(f\"  -> ❌ Discarded question that failed verification.\")\n",
    "                else:\n",
    "                    print(f\"  -> ❌ Discarded question with low quality score: {quality_score}\")\n",
    "            else:\n",
    "                print(\"  -> ❌ Discarded question that failed pre-filter.\")\n",
    "        else:\n",
    "            print(\"  -> No relevant passages found, skipping.\")\n",
    "\n",
    "    return generated_questions\n",
    "\n",
    "# --- EXECUTE ---\n",
    "if os.path.exists(PDF_FILE_PATH):\n",
    "    # --- NEW CACHING LOGIC ---\n",
    "    # Create a unique cache file name based on the PDF name\n",
    "    cache_filename = os.path.basename(PDF_FILE_PATH) + \".pkl\"\n",
    "    \n",
    "    if os.path.exists(cache_filename):\n",
    "        print(f\"Loading text chunks and embeddings from cache file: {cache_filename}\")\n",
    "        with open(cache_filename, \"rb\") as f:\n",
    "            text_chunks, chunk_embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cache found. Processing PDF from scratch...\")\n",
    "        document_text = extract_text_from_pdf(PDF_FILE_PATH)\n",
    "        text_chunks = chunk_text(document_text)\n",
    "        print(\"Creating embeddings for document chunks...\")\n",
    "        chunk_embeddings = embedding_model.encode(text_chunks, convert_to_tensor=True)\n",
    "        print(\"Embeddings created.\")\n",
    "        \n",
    "        print(f\"Saving chunks and embeddings to cache file: {cache_filename}\")\n",
    "        with open(cache_filename, \"wb\") as f:\n",
    "            pickle.dump((text_chunks, chunk_embeddings), f)\n",
    "    # --- END OF CACHING LOGIC ---\n",
    "\n",
    "    # Now, run the pipeline with the processed data\n",
    "    final_quiz = run_full_pipeline(text_chunks, chunk_embeddings, client)\n",
    "\n",
    "    # Save the final list of questions to a JSON file\n",
    "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
    "        json.dump(final_quiz, f, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ Pipeline complete! Generated {len(final_quiz)} questions.\")\n",
    "    print(f\"Output saved to {OUTPUT_JSON_PATH}\")\n",
    "else:\n",
    "    print(f\"❌ Error: The file '{PDF_FILE_PATH}' was not found. Please update the path in Cell 2.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
