[
    {
        "question": "What is the function of a derivative in describing the growth or decrease of a function?",
        "choices": [
            "Describes how fast a function grows or decreases",
            "Determines the area under the curve of a function",
            "Calculates the average value of a function",
            "Measures the variability of a function"
        ],
        "correct_answer": "Describes how fast a function grows or decreases",
        "explanation": "A derivative of a function describes how fast the function grows or decreases at any point in its domain. It provides information about the rate of change of the function.",
        "difficulty": "easy",
        "quality_score": 4,
        "source_passages": [
            "2.1.9 Derivative and Gradient A derivative f \u2260of a function f is a function or a value that describes how fast f grows (or decreases). If the derivative is a constant value, like 5 or \u22603, then the function grows (or decreases) constantly at any point x of its domain. If the derivative f \u2260is a function, then the function f can grow at a di\ufb00erent pace in di\ufb00erent regions of its domain. If the derivative f \u2260 is positive at some point x, then the function f grows at this point. If the derivative of f is negative at some x, then the function decreases at this point. The derivative of zero at x means that the function\u2019s slope at x is horizontal. The process of \ufb01nding a derivative is called di\ufb00erentiation. Derivatives for basic functions are known. For example if f(x) = x2, then f \u2260(x) = 2x; if f(x) = 2x then f \u2260(x) = 2; if f(x) = 2 then f \u2260(x) = 0 (the derivative of any function f(x) = c, where c is a constant value, is zero). If the function we want to di\ufb00erentiate is not basic, we can \ufb01nd its derivative using the chain rule. For example if F(x) = f(g(x)), where f and g are some functions, then F \u2260(x) = f \u2260(g(x))g\u2260(x). For example if F(x) = (5x + 1)2 then g(x) = 5x + 1 and f(g(x)) = (g(x))2. By applying the chain rule, we \ufb01nd F \u2260(x) = 2(5x + 1)g\u2260(x) = 2(5x + 1)5 = 50x + 10. Gradient is the generalization of derivative for functions that take several inputs (or one input in the form of a vector or some other complex structure). A gradient of a function is a vector of partial derivatives. You can look at \ufb01nding a partial derivative of a function as the process of \ufb01nding the derivative by focusing on one of the function\u2019s inputs and by considering all other inputs as constant values. For example, if our function is de\ufb01ned as f([x(1), x(2)]) = ax(1) + bx(2) + c, then the partial derivative of function f with respect to x(1), denoted as \u02c6f \u02c6x(1) , is given by, \u02c6f \u02c6x(1) = a + 0 + 0 = a, where a is the derivative of the function ax(1); the two zeroes are respectively derivatives of bx(2) and c, because x(2) is considered constant when we compute the derivative with respect to x(1), and the derivative of any constant is zero. Similarly, the partial derivative of function f with respect to x(2), \u02c6f \u02c6x(2) , is given by, \u02c6f \u02c6x(2) = 0 + b + 0 = b. The gradient of function f, denoted as \u0153f is given by the vector [ \u02c6f \u02c6x(1) , \u02c6f \u02c6x(2) ]. The chain rule works with partial derivatives too, as I illustrate in Chapter 4. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 (a) (b) Figure 3: A probability mass function and a probability density function. 2.2 Random Variable A random variable, usually written as an italic capital letter, like X, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables: discrete and continuous. A discrete random variable takes on only a countable number of distinct values such as red, yellow, blue or 1, 2, 3, . . .. The probability distribution of a discrete random variable is described by a list of probabilities associated with each of its possible values. This list of probabilities is called probability mass function (pmf). For example: Pr(X = red) = 0.3, Pr(X = yellow) = 0.45, Pr(X = blue) = 0.25. Each probability in a probability mass function is a value greater than or equal to 0. The sum of probabilities equals 1 (\ufb01g. 3a). A continuous random variable takes an in\ufb01nite number of possible values in some interval. Examples include height, weight, and time. Because the number of values of a continuous random variable X is in\ufb01nite, the probability Pr(X = c) for any c is 0. Therefore, instead of the list of probabilities, the probability distribution of a continuous random variable (a continuous probability distribution) is described by a probability density function (pdf). The pdf is a function whose codomain is nonnegative and the area under the curve is equal to 1 (\ufb01g. 3b). Let a discrete random variable X have k possible values {xi}k i=1. The expectation of X denoted as E[X] is given by, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 E[X] def = k \u00ff i=1 xi Pr(X = xi) = x1 Pr(X = x1) + x2 Pr(X = x2) + \u00b7 \u00b7 \u00b7 + xk Pr(X = xk), (1) where Pr(X = xi) is the probability that X has the value xi according to the pmf. The expectation of a random variable is also called the mean, average or expected value and is frequently denoted with the letter \u00b5. The expectation is one of the most important statistics of a random variable. Another important statistic is the standard deviation. For a discrete random variable, the standard deviation usually denoted as \u2021 is given by: \u2021 def = \u0178 E[(X \u2260\u00b5)2] = \u0178 Pr(X = x1)(x1 \u2260\u00b5)2 + Pr(X = x2)(x2 \u2260\u00b5)2 + \u00b7 \u00b7 \u00b7 + Pr(X = xk)(xk \u2260\u00b5)2, where \u00b5 = E[X]. The expectation of a continuous random variable X is given by, E[X] def = q R xfX(x) dx, (2) where fX is the pdf of the variable X and 5 R is the integral of function xfX. Integral is an equivalent of the summation over all values of the function when the function has a continuous domain. It equals the area under the curve of the function. The property of the pdf that the area under its curve is 1 mathematically means that 5 R fX(x) dx = 1. Most of the time we don\u2019t know fX, but we can observe some values of X. In machine learning, we call these values examples, and the collection of these examples is called a sample or a dataset. 2.3 Unbiased Estimators Because fX is usually unknown, but we have a sample SX = {xi}N i=1, we often content ourselves not with the true values of statistics of the probability distribution, such as expectation, but with their unbiased estimators. We say that \u02c6\u25ca(SX) is an unbiased estimator of some statistic \u25cacalculated using a sample SX drawn from an unknown probability distribution if \u02c6\u25ca(SX) has the following property: E 6 \u02c6\u25ca(SX) S = \u25ca, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 where \u02c6\u25cais a sample statistic, obtained using a sample SX and not the real statistic \u25cathat can be obtained only knowing X; the expectation is taken over all possible samples drawn from X. Intuitively, this means that if you can have an unlimited number of such samples as SX, and you compute some unbiased estimator, such as \u02c6\u00b5, using each sample, then the average of all these \u02c6\u00b5 equals the real statistic \u00b5 that you would get computed on X. It can be shown that an unbiased estimator of an unknown E[X] (given by either eq. 1 or eq. 2) is given by 1 N UN i=1 xi (called in statistics the sample mean). 2.4 Bayes\u2019 Rule The conditional probability Pr(X = x|Y = y) is the probability of the random variable X to have a speci\ufb01c value x given that another random variable Y has a speci\ufb01c value of y. The Bayes\u2019 Rule (also known as the Bayes\u2019 Theorem) stipulates that: Pr(X = x|Y = y) = Pr(Y = y|X = x) Pr(X = x) Pr(Y = y) . 2.5 Parameter Estimation Bayes\u2019 Rule comes in handy when we have a model of X\u2019s distribution, and this model f\u25cais a function that has some parameters in the form of a vector \u25ca. An example of such a function could be the Gaussian function that has two parameters, \u00b5 and \u2021, and is de\ufb01ned as: f\u25ca(x) = 1 \u03a9 2\ufb01\u20212 e\u00d5 (x\u2260\u00b5)2 2\u20212 , where \u25ca def = [\u00b5, \u2021]. This function has all the properties of a pdf. Therefore, we can use it as a model of an unknown distribution of X. We can update the values of parameters in the vector \u25cafrom the data using the Bayes\u2019 Rule: Pr(\u25ca= \u02c6\u25ca|X = x) \ufb02Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) Pr(X = x) = Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) U \u02dc\u25caPr(X = x|\u25ca= \u02dc\u25ca) . (3) where Pr(X = x|\u25ca= \u02c6\u25ca) def = f\u02c6\u25ca. If we have a sample S of X and the set of possible values for \u25cais \ufb01nite, we can easily estimate Pr(\u25ca= \u02c6\u25ca) by applying Bayes\u2019 Rule iteratively, one example x \ufb01S at a time. The initial value Pr(\u25ca= \u02c6\u25ca) can be guessed such that U \u02c6\u25caPr(\u25ca= \u02c6\u25ca) = 1. This guess of the probabilities for di\ufb00erent \u02c6\u25cais called the prior. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 First, we compute Pr(\u25ca= \u02c6\u25ca|X = x1) for all possible values \u02c6\u25ca. Then, before updating Pr(\u25ca= \u02c6\u25ca|X = x) once again, this time for x = x2 \ufb01S using eq. 3, we replace the prior Pr(\u25ca= \u02c6\u25ca) in eq. 3 by the new estimate Pr(\u25ca= \u02c6\u25ca) \ufb021 N U x\u20acS Pr(\u25ca= \u02c6\u25ca|X = x). The best value of the parameters \u25ca\u0153 given one example is obtained using the principle of maximum-likelihood: \u25ca\u0153 = arg max \u25ca N T i=1 Pr(\u25ca= \u02c6\u25ca|X = xi). (4) If the set of possible values for \u25caisn\u2019t \ufb01nite, then we need to optimize eq. 4 directly using a numerical optimization routine, such as gradient descent, which we consider in Chapter 4. Usually, we optimize the natural logarithm of the right-hand side expression in eq. 4 because the logarithm of a product becomes the sum of logarithms and it\u2019s easier for the machine to work with the sum than with a product1. 2.6 Classi\ufb01cation vs. Regression Classi\ufb01cation is a problem of automatically assigning a label to an unlabeled example. Spam detection is a famous example of classi\ufb01cation. In machine learning, the classi\ufb01cation problem is solved by a classi\ufb01cation learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a number that can be used by the data analyst to deduce the label easily. An example of such a number is a probability. In a classi\ufb01cation problem, a label is a member of a \ufb01nite set of classes. If the size of the set of classes is two (\u201csick\u201d/\u201chealthy\u201d, \u201cspam\u201d/\u201cnot_spam\u201d), we talk about binary classi\ufb01cation (also called binomial in some books). Multiclass classi\ufb01cation (also called multinomial) is a classi\ufb01cation problem with three or more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to",
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the"
        ]
    },
    {
        "question": "What is the primary difference between the optimization criteria of logistic regression and decision tree learning?",
        "choices": [
            "Logistic regression maximizes the likelihood of the training data, while decision tree learning minimizes the entropy of the split",
            "Logistic regression minimizes the entropy of the split, while decision tree learning maximizes the likelihood of the training data",
            "Both logistic regression and decision tree learning maximize the likelihood of the training data",
            "Both logistic regression and decision tree learning minimize the entropy of the split"
        ],
        "correct_answer": "Logistic regression maximizes the likelihood of the training data, while decision tree learning minimizes the entropy of the split",
        "explanation": "In logistic regression, the optimization criterion is to maximize the likelihood of the training data according to the model, while in decision tree learning, the goodness of a split is estimated by minimizing the entropy of the split.",
        "difficulty": "medium",
        "quality_score": 5,
        "source_passages": [
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14",
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the"
        ]
    },
    {
        "question": "What is the primary difference between the optimization criterion used in logistic regression and the one used in ID3 decision tree learning?",
        "choices": [
            "Logistic regression maximizes the likelihood of the training data, while ID3 decision tree learning minimizes the entropy of the split",
            "Logistic regression minimizes the entropy of the split, while ID3 decision tree learning maximizes the likelihood of the training data",
            "Both logistic regression and ID3 decision tree learning maximize the likelihood of the training data",
            "Both logistic regression and ID3 decision tree learning minimize the entropy of the split"
        ],
        "correct_answer": "Logistic regression maximizes the likelihood of the training data, while ID3 decision tree learning minimizes the entropy of the split",
        "explanation": "In logistic regression, the optimization criterion is to maximize the likelihood of the training data according to the model, while in ID3 decision tree learning, the criterion is to minimize the entropy of the split at each step.",
        "difficulty": "hard",
        "quality_score": 5,
        "source_passages": [
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14",
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the"
        ]
    },
    {
        "question": "What is the key difference between model-based learning algorithms and instance-based learning algorithms?",
        "choices": [
            "Model-based algorithms create a model using training data, while instance-based algorithms use the entire dataset as the model.",
            "Model-based algorithms use the whole dataset as the model, while instance-based algorithms create a model with parameters learned from training data.",
            "Model-based algorithms predict labels based on the closest neighbors, while instance-based algorithms create a model using the training data.",
            "Model-based algorithms build neural networks with more than one layer, while instance-based algorithms use a linear combination of features."
        ],
        "correct_answer": "Model-based algorithms create a model using training data, while instance-based algorithms use the entire dataset as the model.",
        "explanation": "Model-based learning algorithms, like SVM, create a model using the training data to learn parameters. In contrast, instance-based learning algorithms, like k-Nearest Neighbors, use the entire dataset as the model to predict labels based on the closest neighbors.",
        "difficulty": "easy",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is the key difference between model-based learning algorithms and instance-based learning algorithms?",
        "choices": [
            "Model-based algorithms create a model with learned parameters, while instance-based algorithms use the entire dataset as the model.",
            "Model-based algorithms use the entire dataset as the model, while instance-based algorithms create a model with learned parameters.",
            "Model-based algorithms predict labels based on the closest neighbors, while instance-based algorithms use parameters learned from the training data.",
            "Model-based algorithms discard the training data after model creation, while instance-based algorithms continuously update the model with new data."
        ],
        "correct_answer": "Model-based algorithms create a model with learned parameters, while instance-based algorithms use the entire dataset as the model.",
        "explanation": "Model-based learning algorithms, like SVM, create a model with learned parameters from the training data and discard the data afterward. In contrast, instance-based learning algorithms, like k-Nearest Neighbors, use the entire dataset as the model to predict labels based on the closest neighbors.",
        "difficulty": "medium",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is the key difference between the hyperplane in linear regression and the hyperplane in SVM?",
        "choices": [
            "The hyperplane in linear regression is chosen to be as close to all training examples as possible, while the hyperplane in SVM is used to separate two groups of examples from one another.",
            "The hyperplane in linear regression is used to separate two groups of examples from one another, while the hyperplane in SVM is chosen to be as close to all training examples as possible.",
            "The hyperplane in linear regression is used to maximize the margin between classes, while the hyperplane in SVM is used to minimize the loss function.",
            "The hyperplane in linear regression is chosen based on the average loss, while the hyperplane in SVM is chosen based on the hinge loss function."
        ],
        "correct_answer": "The hyperplane in linear regression is chosen to be as close to all training examples as possible, while the hyperplane in SVM is used to separate two groups of examples from one another.",
        "explanation": "In linear regression, the hyperplane is chosen to be as close to all training examples as possible to predict the value of the target accurately. In contrast, in SVM, the hyperplane is used as a decision boundary to separate two groups of examples from one another.",
        "difficulty": "hard",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What distinguishes model-based learning algorithms from instance-based learning algorithms?",
        "choices": [
            "Model-based algorithms create a model with learned parameters from training data, while instance-based algorithms use the whole dataset as the model.",
            "Model-based algorithms use the whole dataset as the model, while instance-based algorithms create a model with learned parameters from training data.",
            "Model-based algorithms predict labels based on the closest neighbors, while instance-based algorithms create a model with learned parameters from training data.",
            "Model-based algorithms use the whole dataset as the model, while instance-based algorithms predict labels based on the closest neighbors."
        ],
        "correct_answer": "Model-based algorithms create a model with learned parameters from training data, while instance-based algorithms use the whole dataset as the model.",
        "explanation": "Model-based learning algorithms, such as SVM, create a model with parameters learned from training data, which can be discarded after model creation. In contrast, instance-based learning algorithms, like k-Nearest Neighbors, use the whole dataset as the model for prediction.",
        "difficulty": "easy",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is a key difference between model-based learning algorithms and instance-based learning algorithms?",
        "choices": [
            "Model-based algorithms use the training data to create a model with learned parameters, while instance-based algorithms use the entire dataset as the model.",
            "Model-based algorithms discard the training data after building the model, while instance-based algorithms retain the training data as part of the model.",
            "Model-based algorithms focus on minimizing the loss function, while instance-based algorithms prioritize maximizing the margin.",
            "Model-based algorithms require a large number of hyperparameters, while instance-based algorithms have minimal hyperparameter requirements."
        ],
        "correct_answer": "Model-based algorithms use the training data to create a model with learned parameters, while instance-based algorithms use the entire dataset as the model.",
        "explanation": "The source text explains that model-based learning algorithms, like SVM, create a model with learned parameters from the training data, which can then be discarded. In contrast, instance-based learning algorithms, like k-Nearest Neighbors, use the entire dataset as the model to make predictions.",
        "difficulty": "medium",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is the key difference between the hyperplane in linear regression and the hyperplane in support vector machines (SVM)?",
        "choices": [
            "The hyperplane in linear regression separates two groups of examples, while the hyperplane in SVM is used to minimize the distance to all training examples.",
            "The hyperplane in linear regression is chosen to be as far from each group as possible, while the hyperplane in SVM is chosen to be as close to all training examples as possible.",
            "The hyperplane in linear regression is used to maximize the margin between classes, while the hyperplane in SVM is used to minimize the margin between classes.",
            "The hyperplane in linear regression is determined by the loss function, while the hyperplane in SVM is determined by the kernel function."
        ],
        "correct_answer": "The hyperplane in linear regression is chosen to be as close to all training examples as possible.",
        "explanation": "In linear regression, the hyperplane is selected to be as close to all training examples as possible to minimize the error in predicting the target values. On the other hand, in SVM, the hyperplane is chosen to maximize the margin between classes by being equidistant from the closest examples of each class.",
        "difficulty": "hard",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is the purpose of logistic regression in machine learning?",
        "choices": [
            "To model binary classification problems",
            "To model continuous regression problems",
            "To optimize the likelihood of the training set",
            "To minimize the empirical risk"
        ],
        "correct_answer": "To model binary classification problems",
        "explanation": "Logistic regression is specifically designed for classification tasks, where the goal is to predict discrete outcomes (e.g., binary labels) based on input features. It is not used for regression problems where the goal is to predict continuous values.",
        "difficulty": "easy",
        "quality_score": 4,
        "source_passages": [
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the",
            "2.1.9 Derivative and Gradient A derivative f \u2260of a function f is a function or a value that describes how fast f grows (or decreases). If the derivative is a constant value, like 5 or \u22603, then the function grows (or decreases) constantly at any point x of its domain. If the derivative f \u2260is a function, then the function f can grow at a di\ufb00erent pace in di\ufb00erent regions of its domain. If the derivative f \u2260 is positive at some point x, then the function f grows at this point. If the derivative of f is negative at some x, then the function decreases at this point. The derivative of zero at x means that the function\u2019s slope at x is horizontal. The process of \ufb01nding a derivative is called di\ufb00erentiation. Derivatives for basic functions are known. For example if f(x) = x2, then f \u2260(x) = 2x; if f(x) = 2x then f \u2260(x) = 2; if f(x) = 2 then f \u2260(x) = 0 (the derivative of any function f(x) = c, where c is a constant value, is zero). If the function we want to di\ufb00erentiate is not basic, we can \ufb01nd its derivative using the chain rule. For example if F(x) = f(g(x)), where f and g are some functions, then F \u2260(x) = f \u2260(g(x))g\u2260(x). For example if F(x) = (5x + 1)2 then g(x) = 5x + 1 and f(g(x)) = (g(x))2. By applying the chain rule, we \ufb01nd F \u2260(x) = 2(5x + 1)g\u2260(x) = 2(5x + 1)5 = 50x + 10. Gradient is the generalization of derivative for functions that take several inputs (or one input in the form of a vector or some other complex structure). A gradient of a function is a vector of partial derivatives. You can look at \ufb01nding a partial derivative of a function as the process of \ufb01nding the derivative by focusing on one of the function\u2019s inputs and by considering all other inputs as constant values. For example, if our function is de\ufb01ned as f([x(1), x(2)]) = ax(1) + bx(2) + c, then the partial derivative of function f with respect to x(1), denoted as \u02c6f \u02c6x(1) , is given by, \u02c6f \u02c6x(1) = a + 0 + 0 = a, where a is the derivative of the function ax(1); the two zeroes are respectively derivatives of bx(2) and c, because x(2) is considered constant when we compute the derivative with respect to x(1), and the derivative of any constant is zero. Similarly, the partial derivative of function f with respect to x(2), \u02c6f \u02c6x(2) , is given by, \u02c6f \u02c6x(2) = 0 + b + 0 = b. The gradient of function f, denoted as \u0153f is given by the vector [ \u02c6f \u02c6x(1) , \u02c6f \u02c6x(2) ]. The chain rule works with partial derivatives too, as I illustrate in Chapter 4. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 (a) (b) Figure 3: A probability mass function and a probability density function. 2.2 Random Variable A random variable, usually written as an italic capital letter, like X, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables: discrete and continuous. A discrete random variable takes on only a countable number of distinct values such as red, yellow, blue or 1, 2, 3, . . .. The probability distribution of a discrete random variable is described by a list of probabilities associated with each of its possible values. This list of probabilities is called probability mass function (pmf). For example: Pr(X = red) = 0.3, Pr(X = yellow) = 0.45, Pr(X = blue) = 0.25. Each probability in a probability mass function is a value greater than or equal to 0. The sum of probabilities equals 1 (\ufb01g. 3a). A continuous random variable takes an in\ufb01nite number of possible values in some interval. Examples include height, weight, and time. Because the number of values of a continuous random variable X is in\ufb01nite, the probability Pr(X = c) for any c is 0. Therefore, instead of the list of probabilities, the probability distribution of a continuous random variable (a continuous probability distribution) is described by a probability density function (pdf). The pdf is a function whose codomain is nonnegative and the area under the curve is equal to 1 (\ufb01g. 3b). Let a discrete random variable X have k possible values {xi}k i=1. The expectation of X denoted as E[X] is given by, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 E[X] def = k \u00ff i=1 xi Pr(X = xi) = x1 Pr(X = x1) + x2 Pr(X = x2) + \u00b7 \u00b7 \u00b7 + xk Pr(X = xk), (1) where Pr(X = xi) is the probability that X has the value xi according to the pmf. The expectation of a random variable is also called the mean, average or expected value and is frequently denoted with the letter \u00b5. The expectation is one of the most important statistics of a random variable. Another important statistic is the standard deviation. For a discrete random variable, the standard deviation usually denoted as \u2021 is given by: \u2021 def = \u0178 E[(X \u2260\u00b5)2] = \u0178 Pr(X = x1)(x1 \u2260\u00b5)2 + Pr(X = x2)(x2 \u2260\u00b5)2 + \u00b7 \u00b7 \u00b7 + Pr(X = xk)(xk \u2260\u00b5)2, where \u00b5 = E[X]. The expectation of a continuous random variable X is given by, E[X] def = q R xfX(x) dx, (2) where fX is the pdf of the variable X and 5 R is the integral of function xfX. Integral is an equivalent of the summation over all values of the function when the function has a continuous domain. It equals the area under the curve of the function. The property of the pdf that the area under its curve is 1 mathematically means that 5 R fX(x) dx = 1. Most of the time we don\u2019t know fX, but we can observe some values of X. In machine learning, we call these values examples, and the collection of these examples is called a sample or a dataset. 2.3 Unbiased Estimators Because fX is usually unknown, but we have a sample SX = {xi}N i=1, we often content ourselves not with the true values of statistics of the probability distribution, such as expectation, but with their unbiased estimators. We say that \u02c6\u25ca(SX) is an unbiased estimator of some statistic \u25cacalculated using a sample SX drawn from an unknown probability distribution if \u02c6\u25ca(SX) has the following property: E 6 \u02c6\u25ca(SX) S = \u25ca, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 where \u02c6\u25cais a sample statistic, obtained using a sample SX and not the real statistic \u25cathat can be obtained only knowing X; the expectation is taken over all possible samples drawn from X. Intuitively, this means that if you can have an unlimited number of such samples as SX, and you compute some unbiased estimator, such as \u02c6\u00b5, using each sample, then the average of all these \u02c6\u00b5 equals the real statistic \u00b5 that you would get computed on X. It can be shown that an unbiased estimator of an unknown E[X] (given by either eq. 1 or eq. 2) is given by 1 N UN i=1 xi (called in statistics the sample mean). 2.4 Bayes\u2019 Rule The conditional probability Pr(X = x|Y = y) is the probability of the random variable X to have a speci\ufb01c value x given that another random variable Y has a speci\ufb01c value of y. The Bayes\u2019 Rule (also known as the Bayes\u2019 Theorem) stipulates that: Pr(X = x|Y = y) = Pr(Y = y|X = x) Pr(X = x) Pr(Y = y) . 2.5 Parameter Estimation Bayes\u2019 Rule comes in handy when we have a model of X\u2019s distribution, and this model f\u25cais a function that has some parameters in the form of a vector \u25ca. An example of such a function could be the Gaussian function that has two parameters, \u00b5 and \u2021, and is de\ufb01ned as: f\u25ca(x) = 1 \u03a9 2\ufb01\u20212 e\u00d5 (x\u2260\u00b5)2 2\u20212 , where \u25ca def = [\u00b5, \u2021]. This function has all the properties of a pdf. Therefore, we can use it as a model of an unknown distribution of X. We can update the values of parameters in the vector \u25cafrom the data using the Bayes\u2019 Rule: Pr(\u25ca= \u02c6\u25ca|X = x) \ufb02Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) Pr(X = x) = Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) U \u02dc\u25caPr(X = x|\u25ca= \u02dc\u25ca) . (3) where Pr(X = x|\u25ca= \u02c6\u25ca) def = f\u02c6\u25ca. If we have a sample S of X and the set of possible values for \u25cais \ufb01nite, we can easily estimate Pr(\u25ca= \u02c6\u25ca) by applying Bayes\u2019 Rule iteratively, one example x \ufb01S at a time. The initial value Pr(\u25ca= \u02c6\u25ca) can be guessed such that U \u02c6\u25caPr(\u25ca= \u02c6\u25ca) = 1. This guess of the probabilities for di\ufb00erent \u02c6\u25cais called the prior. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 First, we compute Pr(\u25ca= \u02c6\u25ca|X = x1) for all possible values \u02c6\u25ca. Then, before updating Pr(\u25ca= \u02c6\u25ca|X = x) once again, this time for x = x2 \ufb01S using eq. 3, we replace the prior Pr(\u25ca= \u02c6\u25ca) in eq. 3 by the new estimate Pr(\u25ca= \u02c6\u25ca) \ufb021 N U x\u20acS Pr(\u25ca= \u02c6\u25ca|X = x). The best value of the parameters \u25ca\u0153 given one example is obtained using the principle of maximum-likelihood: \u25ca\u0153 = arg max \u25ca N T i=1 Pr(\u25ca= \u02c6\u25ca|X = xi). (4) If the set of possible values for \u25caisn\u2019t \ufb01nite, then we need to optimize eq. 4 directly using a numerical optimization routine, such as gradient descent, which we consider in Chapter 4. Usually, we optimize the natural logarithm of the right-hand side expression in eq. 4 because the logarithm of a product becomes the sum of logarithms and it\u2019s easier for the machine to work with the sum than with a product1. 2.6 Classi\ufb01cation vs. Regression Classi\ufb01cation is a problem of automatically assigning a label to an unlabeled example. Spam detection is a famous example of classi\ufb01cation. In machine learning, the classi\ufb01cation problem is solved by a classi\ufb01cation learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a number that can be used by the data analyst to deduce the label easily. An example of such a number is a probability. In a classi\ufb01cation problem, a label is a member of a \ufb01nite set of classes. If the size of the set of classes is two (\u201csick\u201d/\u201chealthy\u201d, \u201cspam\u201d/\u201cnot_spam\u201d), we talk about binary classi\ufb01cation (also called binomial in some books). Multiclass classi\ufb01cation (also called multinomial) is a classi\ufb01cation problem with three or more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to"
        ]
    },
    {
        "question": "What is the key difference between the optimization criterion for logistic regression and decision tree learning?",
        "choices": [
            "Logistic regression maximizes the likelihood of the training set, while decision tree learning minimizes the average log-likelihood",
            "Logistic regression minimizes the empirical risk, while decision tree learning maximizes the likelihood of the training set",
            "Logistic regression minimizes the likelihood of the training set, while decision tree learning maximizes the average log-likelihood",
            "Logistic regression maximizes the empirical risk, while decision tree learning minimizes the likelihood of the training set"
        ],
        "correct_answer": "Logistic regression maximizes the likelihood of the training set, while decision tree learning minimizes the average log-likelihood",
        "explanation": "The optimization criterion in logistic regression is to maximize the likelihood of the training data according to the model, while decision tree learning aims to minimize the average log-likelihood as part of the ID3 algorithm.",
        "difficulty": "medium",
        "quality_score": 5,
        "source_passages": [
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is the primary difference between the optimization criteria of logistic regression and decision tree learning?",
        "choices": [
            "Logistic regression maximizes the likelihood of the training data, while decision tree learning maximizes the average log-likelihood criterion.",
            "Logistic regression minimizes the average loss, while decision tree learning minimizes the entropy of the split.",
            "Logistic regression optimizes a parametric model, while decision tree learning constructs a non-parametric model.",
            "Logistic regression uses a squared loss function, while decision tree learning uses a hinge loss function."
        ],
        "correct_answer": "Logistic regression maximizes the likelihood of the training data, while decision tree learning maximizes the average log-likelihood criterion.",
        "explanation": "In logistic regression, the optimization criterion is to maximize the likelihood of the training data according to the model, while in decision tree learning, the optimization criterion is to maximize the average log-likelihood criterion.",
        "difficulty": "hard",
        "quality_score": 4,
        "source_passages": [
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14",
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the"
        ]
    },
    {
        "question": "What is the primary difference between the optimization criteria used in logistic regression and decision tree learning?",
        "choices": [
            "Logistic regression optimizes the likelihood of the training data, while decision tree learning optimizes the average log-likelihood.",
            "Logistic regression optimizes the average log-likelihood, while decision tree learning optimizes the likelihood of the training data.",
            "Logistic regression optimizes the squared error loss, while decision tree learning optimizes the entropy of the dataset.",
            "Logistic regression optimizes the entropy of the dataset, while decision tree learning optimizes the squared error loss."
        ],
        "correct_answer": "Logistic regression optimizes the likelihood of the training data, while decision tree learning optimizes the average log-likelihood.",
        "explanation": "In logistic regression, the optimization criterion is the likelihood of the training data according to the model, while in decision tree learning, the optimization criterion is the average log-likelihood.",
        "difficulty": "medium",
        "quality_score": 5,
        "source_passages": [
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14",
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the"
        ]
    },
    {
        "question": "What is the key difference between the hyperplane in linear regression and the hyperplane in support vector machine (SVM)?",
        "choices": [
            "The hyperplane in linear regression is chosen to be as close to all training examples as possible, while the hyperplane in SVM is used to separate two groups of examples from one another.",
            "The hyperplane in linear regression is used to separate two groups of examples from one another, while the hyperplane in SVM is chosen to be as close to all training examples as possible.",
            "The hyperplane in linear regression is used to maximize the margin between classes, while the hyperplane in SVM is chosen to minimize the loss function.",
            "The hyperplane in linear regression is used to minimize the loss function, while the hyperplane in SVM is used to maximize the margin between classes."
        ],
        "correct_answer": "The hyperplane in linear regression is chosen to be as close to all training examples as possible, while the hyperplane in SVM is used to separate two groups of examples from one another.",
        "explanation": "In linear regression, the hyperplane is chosen to be as close to all training examples as possible to make accurate predictions, while in SVM, the hyperplane is used as a decision boundary to separate two groups of examples from each other.",
        "difficulty": "medium",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is the purpose of the logistic regression model in machine learning?",
        "choices": [
            "To model a linear function of features for binary classification",
            "To predict real-valued labels for regression tasks",
            "To build a decision tree for classification",
            "To optimize the likelihood of the training data according to the model"
        ],
        "correct_answer": "To model a linear function of features for binary classification",
        "explanation": "The logistic regression model is used to model a linear function of features for binary classification tasks, where the output is a probability that an example belongs to a certain class based on the input features.",
        "difficulty": "easy",
        "quality_score": 4,
        "source_passages": [
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the",
            "2.1.9 Derivative and Gradient A derivative f \u2260of a function f is a function or a value that describes how fast f grows (or decreases). If the derivative is a constant value, like 5 or \u22603, then the function grows (or decreases) constantly at any point x of its domain. If the derivative f \u2260is a function, then the function f can grow at a di\ufb00erent pace in di\ufb00erent regions of its domain. If the derivative f \u2260 is positive at some point x, then the function f grows at this point. If the derivative of f is negative at some x, then the function decreases at this point. The derivative of zero at x means that the function\u2019s slope at x is horizontal. The process of \ufb01nding a derivative is called di\ufb00erentiation. Derivatives for basic functions are known. For example if f(x) = x2, then f \u2260(x) = 2x; if f(x) = 2x then f \u2260(x) = 2; if f(x) = 2 then f \u2260(x) = 0 (the derivative of any function f(x) = c, where c is a constant value, is zero). If the function we want to di\ufb00erentiate is not basic, we can \ufb01nd its derivative using the chain rule. For example if F(x) = f(g(x)), where f and g are some functions, then F \u2260(x) = f \u2260(g(x))g\u2260(x). For example if F(x) = (5x + 1)2 then g(x) = 5x + 1 and f(g(x)) = (g(x))2. By applying the chain rule, we \ufb01nd F \u2260(x) = 2(5x + 1)g\u2260(x) = 2(5x + 1)5 = 50x + 10. Gradient is the generalization of derivative for functions that take several inputs (or one input in the form of a vector or some other complex structure). A gradient of a function is a vector of partial derivatives. You can look at \ufb01nding a partial derivative of a function as the process of \ufb01nding the derivative by focusing on one of the function\u2019s inputs and by considering all other inputs as constant values. For example, if our function is de\ufb01ned as f([x(1), x(2)]) = ax(1) + bx(2) + c, then the partial derivative of function f with respect to x(1), denoted as \u02c6f \u02c6x(1) , is given by, \u02c6f \u02c6x(1) = a + 0 + 0 = a, where a is the derivative of the function ax(1); the two zeroes are respectively derivatives of bx(2) and c, because x(2) is considered constant when we compute the derivative with respect to x(1), and the derivative of any constant is zero. Similarly, the partial derivative of function f with respect to x(2), \u02c6f \u02c6x(2) , is given by, \u02c6f \u02c6x(2) = 0 + b + 0 = b. The gradient of function f, denoted as \u0153f is given by the vector [ \u02c6f \u02c6x(1) , \u02c6f \u02c6x(2) ]. The chain rule works with partial derivatives too, as I illustrate in Chapter 4. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 (a) (b) Figure 3: A probability mass function and a probability density function. 2.2 Random Variable A random variable, usually written as an italic capital letter, like X, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables: discrete and continuous. A discrete random variable takes on only a countable number of distinct values such as red, yellow, blue or 1, 2, 3, . . .. The probability distribution of a discrete random variable is described by a list of probabilities associated with each of its possible values. This list of probabilities is called probability mass function (pmf). For example: Pr(X = red) = 0.3, Pr(X = yellow) = 0.45, Pr(X = blue) = 0.25. Each probability in a probability mass function is a value greater than or equal to 0. The sum of probabilities equals 1 (\ufb01g. 3a). A continuous random variable takes an in\ufb01nite number of possible values in some interval. Examples include height, weight, and time. Because the number of values of a continuous random variable X is in\ufb01nite, the probability Pr(X = c) for any c is 0. Therefore, instead of the list of probabilities, the probability distribution of a continuous random variable (a continuous probability distribution) is described by a probability density function (pdf). The pdf is a function whose codomain is nonnegative and the area under the curve is equal to 1 (\ufb01g. 3b). Let a discrete random variable X have k possible values {xi}k i=1. The expectation of X denoted as E[X] is given by, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 E[X] def = k \u00ff i=1 xi Pr(X = xi) = x1 Pr(X = x1) + x2 Pr(X = x2) + \u00b7 \u00b7 \u00b7 + xk Pr(X = xk), (1) where Pr(X = xi) is the probability that X has the value xi according to the pmf. The expectation of a random variable is also called the mean, average or expected value and is frequently denoted with the letter \u00b5. The expectation is one of the most important statistics of a random variable. Another important statistic is the standard deviation. For a discrete random variable, the standard deviation usually denoted as \u2021 is given by: \u2021 def = \u0178 E[(X \u2260\u00b5)2] = \u0178 Pr(X = x1)(x1 \u2260\u00b5)2 + Pr(X = x2)(x2 \u2260\u00b5)2 + \u00b7 \u00b7 \u00b7 + Pr(X = xk)(xk \u2260\u00b5)2, where \u00b5 = E[X]. The expectation of a continuous random variable X is given by, E[X] def = q R xfX(x) dx, (2) where fX is the pdf of the variable X and 5 R is the integral of function xfX. Integral is an equivalent of the summation over all values of the function when the function has a continuous domain. It equals the area under the curve of the function. The property of the pdf that the area under its curve is 1 mathematically means that 5 R fX(x) dx = 1. Most of the time we don\u2019t know fX, but we can observe some values of X. In machine learning, we call these values examples, and the collection of these examples is called a sample or a dataset. 2.3 Unbiased Estimators Because fX is usually unknown, but we have a sample SX = {xi}N i=1, we often content ourselves not with the true values of statistics of the probability distribution, such as expectation, but with their unbiased estimators. We say that \u02c6\u25ca(SX) is an unbiased estimator of some statistic \u25cacalculated using a sample SX drawn from an unknown probability distribution if \u02c6\u25ca(SX) has the following property: E 6 \u02c6\u25ca(SX) S = \u25ca, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 where \u02c6\u25cais a sample statistic, obtained using a sample SX and not the real statistic \u25cathat can be obtained only knowing X; the expectation is taken over all possible samples drawn from X. Intuitively, this means that if you can have an unlimited number of such samples as SX, and you compute some unbiased estimator, such as \u02c6\u00b5, using each sample, then the average of all these \u02c6\u00b5 equals the real statistic \u00b5 that you would get computed on X. It can be shown that an unbiased estimator of an unknown E[X] (given by either eq. 1 or eq. 2) is given by 1 N UN i=1 xi (called in statistics the sample mean). 2.4 Bayes\u2019 Rule The conditional probability Pr(X = x|Y = y) is the probability of the random variable X to have a speci\ufb01c value x given that another random variable Y has a speci\ufb01c value of y. The Bayes\u2019 Rule (also known as the Bayes\u2019 Theorem) stipulates that: Pr(X = x|Y = y) = Pr(Y = y|X = x) Pr(X = x) Pr(Y = y) . 2.5 Parameter Estimation Bayes\u2019 Rule comes in handy when we have a model of X\u2019s distribution, and this model f\u25cais a function that has some parameters in the form of a vector \u25ca. An example of such a function could be the Gaussian function that has two parameters, \u00b5 and \u2021, and is de\ufb01ned as: f\u25ca(x) = 1 \u03a9 2\ufb01\u20212 e\u00d5 (x\u2260\u00b5)2 2\u20212 , where \u25ca def = [\u00b5, \u2021]. This function has all the properties of a pdf. Therefore, we can use it as a model of an unknown distribution of X. We can update the values of parameters in the vector \u25cafrom the data using the Bayes\u2019 Rule: Pr(\u25ca= \u02c6\u25ca|X = x) \ufb02Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) Pr(X = x) = Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) U \u02dc\u25caPr(X = x|\u25ca= \u02dc\u25ca) . (3) where Pr(X = x|\u25ca= \u02c6\u25ca) def = f\u02c6\u25ca. If we have a sample S of X and the set of possible values for \u25cais \ufb01nite, we can easily estimate Pr(\u25ca= \u02c6\u25ca) by applying Bayes\u2019 Rule iteratively, one example x \ufb01S at a time. The initial value Pr(\u25ca= \u02c6\u25ca) can be guessed such that U \u02c6\u25caPr(\u25ca= \u02c6\u25ca) = 1. This guess of the probabilities for di\ufb00erent \u02c6\u25cais called the prior. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 First, we compute Pr(\u25ca= \u02c6\u25ca|X = x1) for all possible values \u02c6\u25ca. Then, before updating Pr(\u25ca= \u02c6\u25ca|X = x) once again, this time for x = x2 \ufb01S using eq. 3, we replace the prior Pr(\u25ca= \u02c6\u25ca) in eq. 3 by the new estimate Pr(\u25ca= \u02c6\u25ca) \ufb021 N U x\u20acS Pr(\u25ca= \u02c6\u25ca|X = x). The best value of the parameters \u25ca\u0153 given one example is obtained using the principle of maximum-likelihood: \u25ca\u0153 = arg max \u25ca N T i=1 Pr(\u25ca= \u02c6\u25ca|X = xi). (4) If the set of possible values for \u25caisn\u2019t \ufb01nite, then we need to optimize eq. 4 directly using a numerical optimization routine, such as gradient descent, which we consider in Chapter 4. Usually, we optimize the natural logarithm of the right-hand side expression in eq. 4 because the logarithm of a product becomes the sum of logarithms and it\u2019s easier for the machine to work with the sum than with a product1. 2.6 Classi\ufb01cation vs. Regression Classi\ufb01cation is a problem of automatically assigning a label to an unlabeled example. Spam detection is a famous example of classi\ufb01cation. In machine learning, the classi\ufb01cation problem is solved by a classi\ufb01cation learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a number that can be used by the data analyst to deduce the label easily. An example of such a number is a probability. In a classi\ufb01cation problem, a label is a member of a \ufb01nite set of classes. If the size of the set of classes is two (\u201csick\u201d/\u201chealthy\u201d, \u201cspam\u201d/\u201cnot_spam\u201d), we talk about binary classi\ufb01cation (also called binomial in some books). Multiclass classi\ufb01cation (also called multinomial) is a classi\ufb01cation problem with three or more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to"
        ]
    },
    {
        "question": "What is the key difference between linear regression and logistic regression?",
        "choices": [
            "Linear regression is used for classification while logistic regression is used for regression",
            "Linear regression minimizes the squared error loss while logistic regression maximizes the likelihood of the training data",
            "Linear regression uses a linear combination of features to predict continuous values while logistic regression models the probability of a binary outcome",
            "Linear regression has a closed-form solution while logistic regression requires numerical optimization methods"
        ],
        "correct_answer": "Linear regression uses a linear combination of features to predict continuous values while logistic regression models the probability of a binary outcome",
        "explanation": "The key difference between linear regression and logistic regression is that linear regression is used to predict continuous values by using a linear combination of features, while logistic regression is used for binary classification by modeling the probability of a binary outcome.",
        "difficulty": "hard",
        "quality_score": 4,
        "source_passages": [
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the",
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning"
        ]
    },
    {
        "question": "What is the main difference between logistic regression and linear regression?",
        "choices": [
            "Logistic regression is used for classification, while linear regression is used for regression",
            "Logistic regression uses a sigmoid function, while linear regression uses a linear function",
            "Logistic regression optimizes the likelihood of the training data, while linear regression minimizes the mean squared error",
            "Logistic regression has a binary output, while linear regression has a continuous output"
        ],
        "correct_answer": "Logistic regression is used for classification, while linear regression is used for regression",
        "explanation": "The main difference between logistic regression and linear regression is that logistic regression is a classification learning algorithm, while linear regression is a regression learning algorithm.",
        "difficulty": "easy",
        "quality_score": 4,
        "source_passages": [
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the",
            "2.1.9 Derivative and Gradient A derivative f \u2260of a function f is a function or a value that describes how fast f grows (or decreases). If the derivative is a constant value, like 5 or \u22603, then the function grows (or decreases) constantly at any point x of its domain. If the derivative f \u2260is a function, then the function f can grow at a di\ufb00erent pace in di\ufb00erent regions of its domain. If the derivative f \u2260 is positive at some point x, then the function f grows at this point. If the derivative of f is negative at some x, then the function decreases at this point. The derivative of zero at x means that the function\u2019s slope at x is horizontal. The process of \ufb01nding a derivative is called di\ufb00erentiation. Derivatives for basic functions are known. For example if f(x) = x2, then f \u2260(x) = 2x; if f(x) = 2x then f \u2260(x) = 2; if f(x) = 2 then f \u2260(x) = 0 (the derivative of any function f(x) = c, where c is a constant value, is zero). If the function we want to di\ufb00erentiate is not basic, we can \ufb01nd its derivative using the chain rule. For example if F(x) = f(g(x)), where f and g are some functions, then F \u2260(x) = f \u2260(g(x))g\u2260(x). For example if F(x) = (5x + 1)2 then g(x) = 5x + 1 and f(g(x)) = (g(x))2. By applying the chain rule, we \ufb01nd F \u2260(x) = 2(5x + 1)g\u2260(x) = 2(5x + 1)5 = 50x + 10. Gradient is the generalization of derivative for functions that take several inputs (or one input in the form of a vector or some other complex structure). A gradient of a function is a vector of partial derivatives. You can look at \ufb01nding a partial derivative of a function as the process of \ufb01nding the derivative by focusing on one of the function\u2019s inputs and by considering all other inputs as constant values. For example, if our function is de\ufb01ned as f([x(1), x(2)]) = ax(1) + bx(2) + c, then the partial derivative of function f with respect to x(1), denoted as \u02c6f \u02c6x(1) , is given by, \u02c6f \u02c6x(1) = a + 0 + 0 = a, where a is the derivative of the function ax(1); the two zeroes are respectively derivatives of bx(2) and c, because x(2) is considered constant when we compute the derivative with respect to x(1), and the derivative of any constant is zero. Similarly, the partial derivative of function f with respect to x(2), \u02c6f \u02c6x(2) , is given by, \u02c6f \u02c6x(2) = 0 + b + 0 = b. The gradient of function f, denoted as \u0153f is given by the vector [ \u02c6f \u02c6x(1) , \u02c6f \u02c6x(2) ]. The chain rule works with partial derivatives too, as I illustrate in Chapter 4. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 (a) (b) Figure 3: A probability mass function and a probability density function. 2.2 Random Variable A random variable, usually written as an italic capital letter, like X, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables: discrete and continuous. A discrete random variable takes on only a countable number of distinct values such as red, yellow, blue or 1, 2, 3, . . .. The probability distribution of a discrete random variable is described by a list of probabilities associated with each of its possible values. This list of probabilities is called probability mass function (pmf). For example: Pr(X = red) = 0.3, Pr(X = yellow) = 0.45, Pr(X = blue) = 0.25. Each probability in a probability mass function is a value greater than or equal to 0. The sum of probabilities equals 1 (\ufb01g. 3a). A continuous random variable takes an in\ufb01nite number of possible values in some interval. Examples include height, weight, and time. Because the number of values of a continuous random variable X is in\ufb01nite, the probability Pr(X = c) for any c is 0. Therefore, instead of the list of probabilities, the probability distribution of a continuous random variable (a continuous probability distribution) is described by a probability density function (pdf). The pdf is a function whose codomain is nonnegative and the area under the curve is equal to 1 (\ufb01g. 3b). Let a discrete random variable X have k possible values {xi}k i=1. The expectation of X denoted as E[X] is given by, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 E[X] def = k \u00ff i=1 xi Pr(X = xi) = x1 Pr(X = x1) + x2 Pr(X = x2) + \u00b7 \u00b7 \u00b7 + xk Pr(X = xk), (1) where Pr(X = xi) is the probability that X has the value xi according to the pmf. The expectation of a random variable is also called the mean, average or expected value and is frequently denoted with the letter \u00b5. The expectation is one of the most important statistics of a random variable. Another important statistic is the standard deviation. For a discrete random variable, the standard deviation usually denoted as \u2021 is given by: \u2021 def = \u0178 E[(X \u2260\u00b5)2] = \u0178 Pr(X = x1)(x1 \u2260\u00b5)2 + Pr(X = x2)(x2 \u2260\u00b5)2 + \u00b7 \u00b7 \u00b7 + Pr(X = xk)(xk \u2260\u00b5)2, where \u00b5 = E[X]. The expectation of a continuous random variable X is given by, E[X] def = q R xfX(x) dx, (2) where fX is the pdf of the variable X and 5 R is the integral of function xfX. Integral is an equivalent of the summation over all values of the function when the function has a continuous domain. It equals the area under the curve of the function. The property of the pdf that the area under its curve is 1 mathematically means that 5 R fX(x) dx = 1. Most of the time we don\u2019t know fX, but we can observe some values of X. In machine learning, we call these values examples, and the collection of these examples is called a sample or a dataset. 2.3 Unbiased Estimators Because fX is usually unknown, but we have a sample SX = {xi}N i=1, we often content ourselves not with the true values of statistics of the probability distribution, such as expectation, but with their unbiased estimators. We say that \u02c6\u25ca(SX) is an unbiased estimator of some statistic \u25cacalculated using a sample SX drawn from an unknown probability distribution if \u02c6\u25ca(SX) has the following property: E 6 \u02c6\u25ca(SX) S = \u25ca, Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 where \u02c6\u25cais a sample statistic, obtained using a sample SX and not the real statistic \u25cathat can be obtained only knowing X; the expectation is taken over all possible samples drawn from X. Intuitively, this means that if you can have an unlimited number of such samples as SX, and you compute some unbiased estimator, such as \u02c6\u00b5, using each sample, then the average of all these \u02c6\u00b5 equals the real statistic \u00b5 that you would get computed on X. It can be shown that an unbiased estimator of an unknown E[X] (given by either eq. 1 or eq. 2) is given by 1 N UN i=1 xi (called in statistics the sample mean). 2.4 Bayes\u2019 Rule The conditional probability Pr(X = x|Y = y) is the probability of the random variable X to have a speci\ufb01c value x given that another random variable Y has a speci\ufb01c value of y. The Bayes\u2019 Rule (also known as the Bayes\u2019 Theorem) stipulates that: Pr(X = x|Y = y) = Pr(Y = y|X = x) Pr(X = x) Pr(Y = y) . 2.5 Parameter Estimation Bayes\u2019 Rule comes in handy when we have a model of X\u2019s distribution, and this model f\u25cais a function that has some parameters in the form of a vector \u25ca. An example of such a function could be the Gaussian function that has two parameters, \u00b5 and \u2021, and is de\ufb01ned as: f\u25ca(x) = 1 \u03a9 2\ufb01\u20212 e\u00d5 (x\u2260\u00b5)2 2\u20212 , where \u25ca def = [\u00b5, \u2021]. This function has all the properties of a pdf. Therefore, we can use it as a model of an unknown distribution of X. We can update the values of parameters in the vector \u25cafrom the data using the Bayes\u2019 Rule: Pr(\u25ca= \u02c6\u25ca|X = x) \ufb02Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) Pr(X = x) = Pr(X = x|\u25ca= \u02c6\u25ca) Pr(\u25ca= \u02c6\u25ca) U \u02dc\u25caPr(X = x|\u25ca= \u02dc\u25ca) . (3) where Pr(X = x|\u25ca= \u02c6\u25ca) def = f\u02c6\u25ca. If we have a sample S of X and the set of possible values for \u25cais \ufb01nite, we can easily estimate Pr(\u25ca= \u02c6\u25ca) by applying Bayes\u2019 Rule iteratively, one example x \ufb01S at a time. The initial value Pr(\u25ca= \u02c6\u25ca) can be guessed such that U \u02c6\u25caPr(\u25ca= \u02c6\u25ca) = 1. This guess of the probabilities for di\ufb00erent \u02c6\u25cais called the prior. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 First, we compute Pr(\u25ca= \u02c6\u25ca|X = x1) for all possible values \u02c6\u25ca. Then, before updating Pr(\u25ca= \u02c6\u25ca|X = x) once again, this time for x = x2 \ufb01S using eq. 3, we replace the prior Pr(\u25ca= \u02c6\u25ca) in eq. 3 by the new estimate Pr(\u25ca= \u02c6\u25ca) \ufb021 N U x\u20acS Pr(\u25ca= \u02c6\u25ca|X = x). The best value of the parameters \u25ca\u0153 given one example is obtained using the principle of maximum-likelihood: \u25ca\u0153 = arg max \u25ca N T i=1 Pr(\u25ca= \u02c6\u25ca|X = xi). (4) If the set of possible values for \u25caisn\u2019t \ufb01nite, then we need to optimize eq. 4 directly using a numerical optimization routine, such as gradient descent, which we consider in Chapter 4. Usually, we optimize the natural logarithm of the right-hand side expression in eq. 4 because the logarithm of a product becomes the sum of logarithms and it\u2019s easier for the machine to work with the sum than with a product1. 2.6 Classi\ufb01cation vs. Regression Classi\ufb01cation is a problem of automatically assigning a label to an unlabeled example. Spam detection is a famous example of classi\ufb01cation. In machine learning, the classi\ufb01cation problem is solved by a classi\ufb01cation learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a number that can be used by the data analyst to deduce the label easily. An example of such a number is a probability. In a classi\ufb01cation problem, a label is a member of a \ufb01nite set of classes. If the size of the set of classes is two (\u201csick\u201d/\u201chealthy\u201d, \u201cspam\u201d/\u201cnot_spam\u201d), we talk about binary classi\ufb01cation (also called binomial in some books). Multiclass classi\ufb01cation (also called multinomial) is a classi\ufb01cation problem with three or more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to"
        ]
    },
    {
        "question": "What is the key difference between the hyperplane in Support Vector Machine (SVM) and the hyperplane in linear regression?",
        "choices": [
            "The hyperplane in SVM separates two groups of examples, while the hyperplane in linear regression is chosen to be as close to all training examples as possible.",
            "The hyperplane in SVM is chosen to be as close to all training examples as possible, while the hyperplane in linear regression separates two groups of examples.",
            "The hyperplane in SVM is used to predict the value of the target for new unlabeled input examples, while the hyperplane in linear regression is used to separate two groups of examples.",
            "The hyperplane in SVM exaggerates the difference between the true target and the predicted one, while the hyperplane in linear regression minimizes the difference between the true target and the predicted one."
        ],
        "correct_answer": "The hyperplane in SVM separates two groups of examples, while the hyperplane in linear regression is chosen to be as close to all training examples as possible.",
        "explanation": "In Support Vector Machine (SVM), the hyperplane is used as the decision boundary to separate two groups of examples from each other, while in linear regression, the hyperplane is chosen to be as close to all training examples as possible to make accurate predictions.",
        "difficulty": "medium",
        "quality_score": 4,
        "source_passages": [
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning",
            "The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the data and no hyperplane can perfectly separate positive examples from negative ones? 2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial? Figure 5: Linearly non-separable cases. Left: the presence of noise. Right: inherent nonlinearity. You can see both situations depicted in \ufb01g 5. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). In the right case, the decision boundary is a circle and not a straight line. Remember that in SVM, we want to satisfy the following constraints: a) wxi \u2260b \u00d8 1 if yi = +1, and b) wxi \u2260b \u00c6 \u22601 if yi = \u22601 We also want to minimize \u00cew\u00ce so that the hyperplane was equally distant from the closest examples of each class. Minimizing \u00cew\u00ce is equivalent to minimizing 1 2||w||2, and the use of this term makes it possible to perform quadratic programming optimization later on. The optimization problem for SVM, therefore, looks like this: min 1 2||w||2, such that yi(xiw \u2260b) \u22601 \u00d8 0, i = 1, . . . , N. (8) Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 3.4.1 Dealing with Noise To extend SVM to cases in which the data is not linearly separable, we introduce the hinge loss function: max (0, 1 \u2260yi(wxi \u2260b)). The hinge loss function is zero if the constraints a) and b) are satis\ufb01ed, in other words, if wxi lies on the correct side of the decision boundary. For data on the wrong side of the decision boundary, the function\u2019s value is proportional to the distance from the decision boundary. We then wish to minimize the following cost function, C\u00cew\u00ce2 + 1 N N \u00ff i=1 max (0, 1 \u2260yi(wxi \u2260b)) , where the hyperparameter C determines the tradeo\ufb00between increasing the size of the decision boundary and ensuring that each xi lies on the correct side of the decision boundary. The value of C is usually chosen experimentally, just like ID3\u2019s hyperparameters \u2018 and d. SVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is referred to as a hard-margin SVM. As you can see, for su\ufb03ciently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to \ufb01nd the highest margin by completely ignoring misclassi\ufb01cation. As we decrease the value of C, making classi\ufb01cation errors is becoming more costly, so the SVM algorithm will try to make fewer mistakes by sacri\ufb01cing the margin size. As we have already discussed, a larger margin is better for generalization. Therefore, C regulates the tradeo\ufb00between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization). 3.4.2 Dealing with Inherent Non-Linearity SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. However, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick. The e\ufb00ect of applying the kernel trick is illustrated in \ufb01g. 6. As you can see, it\u2019s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable three- dimensional data using a speci\ufb01c mapping \u201e : x \u2018\u00e6 \u201e(x), where \u201e(x) is a vector of higher dimensionality than x. For the example of 2D data in \ufb01g. 5 (right), the mapping \u201e for example x = [q, p] that projects this example into a 3D space (\ufb01g. 6) would look like this \u201e([q, p]) def = (q2, \u00d4 2qp, p2), where q2 means q squared. You see now that the data becomes linearly separable in the transformed space. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 Figure 6: The data from \ufb01g. 5 (right) becomes linearly separable after a transformation into a three-dimensional space. However, we don\u2019t know a priori which mapping \u201e would work for our data. If we \ufb01rst transform all our input examples using some mapping into very high dimensional vectors and then apply SVM to this data, and we try all possible mapping functions, the computation could become very ine\ufb03cient, and we would never solve our classi\ufb01cation problem. Fortunately, scientists \ufb01gured out how to use kernel functions (or, simply, kernels) to e\ufb03ciently work in higher-dimensional spaces without doing this transformation explicitly. To understand how kernels work, we have to see \ufb01rst how the optimization algorithm for SVM \ufb01nds the optimal values for w and b. The method traditionally used to solve the optimization problem in eq. 8 is the method of Lagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to solve an equivalent problem formulated like this: max \u20131...\u2013N N \u00ff i=1 \u2013i \u22601 2 N \u00ff i=1 N \u00ff k=1 yi\u2013i(xixk)yk\u2013k subject to N \u00ff i=1 \u2013iyi = 0 and \u2013i \u00d8 0, i = 1, . . . , N, where \u2013i are called Lagrange multipliers. When formulated like this, the optimization problem becomes a convex quadratic optimization problem, e\ufb03ciently solvable by quadratic programming algorithms. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 14"
        ]
    },
    {
        "question": "What is the key difference between linear regression and logistic regression in terms of their target variables?",
        "choices": [
            "Linear regression predicts continuous target variables, while logistic regression predicts categorical target variables.",
            "Linear regression predicts categorical target variables, while logistic regression predicts continuous target variables.",
            "Linear regression predicts binary target variables, while logistic regression predicts multiclass target variables.",
            "Linear regression predicts multiclass target variables, while logistic regression predicts binary target variables."
        ],
        "correct_answer": "Linear regression predicts continuous target variables, while logistic regression predicts categorical target variables.",
        "explanation": "Linear regression is used for predicting continuous target variables, while logistic regression is specifically designed for predicting categorical target variables, making it suitable for binary or multiclass classification tasks.",
        "difficulty": "medium",
        "quality_score": 4,
        "source_passages": [
            "then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 6 Figure 3: Standard logistic function. At the time where the absence of computers required scientists to perform manual calculations, they were eager to \ufb01nd a linear classi\ufb01cation model. They \ufb01gured out that if we de\ufb01ne a negative label as 0 and the positive label as 1, we would just need to \ufb01nd a simple continuous function whose codomain is (0, 1). In such a case, if the value returned by the model for input x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled as positive. One function that has such a property is the standard logistic function (also known as the sigmoid function): f(x) = 1 1 + e\u2260x , where e is the base of the natural logarithm (also called Euler\u2019s number; ex is also known as the exp(x) function in Excel and many programming languages). Its graph is depicted in \ufb01g. 3. By looking at the graph of the standard logistic function, we can see how well it \ufb01ts our classi\ufb01cation purpose: if we optimize the values of x and b appropriately, we could interpret the output of f(x) as the probability of yi being positive. For example, if it\u2019s higher than or equal to the threshold 0.5 we would say that the class of x is positive; otherwise, it\u2019s negative. In practice, the choice of the threshold could be di\ufb00erent depending on the problem. We return to this discussion in Chapter 5 when we talk about model performance assessment. So our logistic regression model looks like this: Andriy Burkov The Hundred-Page Machine Learning Book - Draft 7 fw,b(x) def = 1 1 + e\u2260(wx+b) . (3) You can see the familiar term wx + b from linear regression. Now, how do we \ufb01nd the best values w\u00fa and b\u00fa for our model? In linear regression, we minimized the empirical risk which was de\ufb01ned as the average squared error loss, also known as the mean squared error or MSE. 3.2.2 Solution In logistic regression, instead of using a squared loss and trying to minimize the empirical risk, we maximize the likelihood of our training set according to the model. In statistics, the likelihood function de\ufb01nes how likely the observation (an example) is according to our model. For instance, assume that we have a labeled example (xi, yi) in our training data. Assume also that we have found (guessed) some speci\ufb01c values \u02c6w and \u02c6b of our parameters. If we now apply our model f\u02c6w,\u02c6b to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is the positive class, the likelihood of yi being the positive class, according to our model, is given by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is given by 1 \u2260p. The optimization criterion in logistic regression is called maximum likelihood. Instead of minimizing the average loss, like in linear regression, we now maximize the likelihood of the training data according to our model: Lw,b def = \u0178 i=1...N fw,b(xi)yi(1 \u2260fw,b(xi))(1\u2260yi). (4) The expression fw,b(x)yi(1 \u2260fw,b(x))(1\u2260yi) may look scary but it\u2019s just a fancy mathematical way of saying: \u201cfw,b(x) when yi = 1 and (1 \u2260fw,b(x)) otherwise\u201d. Indeed, if yi = 1, then (1 \u2260fw,b(x))(1\u2260yi) equals 1 because (1 \u2260yi) = 0 and we know that anything power 0 equals 1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason. You may have noticed that we used the product operator r in the objective function instead of the sum operator q which was used in linear regression. It\u2019s because the likelihood of observing N labels for N examples is the product of likelihoods of each observation (assuming that all observations are independent of one another, which is the case). You can draw a parallel with the multiplication of probabilities of outcomes in a series of independent experiments in the probability theory. Because of the exp function used in the model, in practice, it\u2019s more convenient to maximize the log-likelihood instead of likelihood. The log-likelihood is de\ufb01ned like follows: LogLw,b def = ln(L(w,b(x)) = N \u00ff i=1 yi ln fw,b(x) + (1 \u2260yi) ln (1 \u2260fw,b(x)). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 8 Because ln is a strictly increasing function, maximizing this function is the same as maximizing its argument, and the solution to this new optimization problem is the same as the solution to the original problem. Contrary to linear regression, there\u2019s no closed form solution to the above optimization problem. A typical numerical optimization procedure used in such cases is gradient descent. I talk about it in the next chapter. 3.3 Decision Tree Learning A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a speci\ufb01c feature j of the feature vector is examined. If the value of the feature is below a speci\ufb01c threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. As the title of the section suggests, a decision tree can be learned from data. 3.3.1 Problem Statement Like previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We want to build a decision tree that would allow us to predict the class of an example given a feature vector. 3.3.2 Solution There are various formulations of the decision tree learning algorithm. In this book, we consider just one, called ID3. The optimization criterion, in this case, is the average log-likelihood: 1 N N \u00ff i=1 yi ln fID3(xi) + (1 \u2260yi) ln (1 \u2260fID3(xi)), (5) where fID3 is a decision tree. By now, it looks very similar to logistic regression. However, contrary to the logistic regression learning algorithm which builds a parametric model fw\u00fa,b\u00fa by \ufb01nding an optimal solution to the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a non-parametric model fID3(x) def = Pr(y = 1|x). Andriy Burkov The Hundred-Page Machine Learning Book - Draft 9 6 ^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f\u0003(x\u0016\u000f\u0003\\\u0016)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f (x\u001a\u000f\u0003\\\u001a)\u000f\u0003(x\u001b\u000f \\\u001b)\u000f\u0003(x \u000f \\ )\u000f (x1\u0013\u000f\u0003\\1\u0013)\u000f\u0003(x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0016\u000e\\\u0017\u000e\\\u0018 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ \u000e\\\u0014\u0013\u000e\\\u0014\u0014\u000e\\\u00142)\u001212 3U(\\ 1_x) (a) x 3U(\\ 1_x) (\\\u0014\u000e\\2\u000e\\\u0017 \u000e\\\u0019\u000e\\\u001a\u000e\\\u001b\u000e\\ )\u0012\u001a 3U(\\ 1_x) x(\u0016)\u0003 \u00031\u001b\u0011\u0016\" 6\u0010 \u0003^(x1\u000f \\1)\u000f (x2\u000f \\2)\u000f (x\u0017\u000f\u0003\\\u0017)\u000f\u0003(x\u0019\u000f\u0003\\\u0019)\u000f\u0003(x\u001a\u000f\u0003\\\u001a)\u000f (x\u001b\u000f\u0003\\\u001b)\u000f\u0003(x \u000f \\ )` 3U(\\ 1_x) (\\\u0016\u000e\\\u0018\u000e\\\u0014\u0013\u000e\\11\u000e\\12)\u0012\u0018 3U(\\ 1_x) 6\u000e \u0003^(x\u0016\u000f\u0003\\\u0016)\u000f\u0003(x\u0018\u000f\u0003\\\u0018)\u000f\u0003(x1\u0013\u000f\u0003\\1\u0013)\u000f (x11\u000f\u0003\\11)\u000f\u0003(x12\u000f\u0003\\12)` <HV 1R (b) Figure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled examples. (a) In the beginning, the decision tree only contains the start node; it makes the same prediction for any input. (b) The decision tree after the \ufb01rst split; it tests whether feature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the two leaf nodes. The ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the beginning, the decision tree only has a start node that contains all examples: S def = {(xi, yi)}N i=1. Start with a constant model f S ID3: f S ID3 = 1 |S| \u00ff (x,y)\u0153S y. (6) The prediction given by the above model, f S ID3(x), would be the same for any input x. The corresponding decision tree is shown in \ufb01g 4a. Then we search through all features j = 1, . . . , D and all thresholds t, and split the set S into two subsets: S\u2260 def = {(x, y) | (x, y) \u0153 S, x(j) < t} and S+ = {(x, y) | (x, y) \u0153 S, x(j) \u00d8 t}. The two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs (j, t) how good the split with pieces S\u2260and S+ is. Finally, we pick the best such values (j, t), split S into S+ and S\u2260, form two new leaf nodes, and continue recursively on S+ and S\u2260(or quit if no split produces a model that\u2019s su\ufb03ciently better than the current one). A decision Andriy Burkov The Hundred-Page Machine Learning Book - Draft 10 tree after one split is illustrated in \ufb01g 4b. Now you should wonder what do the words \u201cevaluate how good the split is\u201d mean. In ID3, the goodness of a split is estimated by using the criterion called entropy. Entropy is a measure of uncertainty about a random variable. It reaches its maximum when all values of the random variables are equiprobable. Entropy reaches its minimum when the random variable can have only one value. The entropy of a set of examples S is given by: H(S) = \u2260f S ID3 ln f S ID3 \u2260(1 \u2260f S ID3) ln(1 \u2260f S ID3). When we split a set of examples by a certain feature j and a threshold t, the entropy of a split, H(S\u2260, S+), is simply a weighted sum of two entropies: H(S\u2260, S+) = |S\u2260| |S| H(S\u2260) + |S+| |S| H(S+). (7) So, in ID3, at each step, at each leaf node, we \ufb01nd a split that minimizes the entropy given by eq. 7 or we stop at this leaf node. The algorithm stops at a leaf node in any of the below situations: \u2022 All examples in the leaf node are classi\ufb01ed correctly by the one-piece model (eq. 6). \u2022 We cannot \ufb01nd an attribute to split upon. \u2022 The split reduces the entropy less than some \u2018 (the value for which has to be found experimentally3). \u2022 The tree reaches some maximum depth d (also has to be found experimentally). Because in ID3, the decision to split the dataset on each iteration is local (doesn\u2019t depend on future splits), the algorithm doesn\u2019t guarantee an optimal solution. The model can be improved by using techniques like backtracking during the search for the optimal decision tree at the cost of possibly taking longer to build a model. The entropy-based split criterion intuitively makes sense: entropy reaches its minimum of 0 when all examples in S have the same label; on the other hand, the entropy is at its maximum of 1 when exactly one-half of examples in S is labeled with 1, making such a leaf useless for classi\ufb01cation. The only remaining question is how this algorithm approximately maximizes the average log-likelihood criterion. I leave it for further reading. 3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 11 3.4 Support Vector Machine We already considered SVM in the introduction, so this section only \ufb01lls a couple of blanks. Two critical questions need to be answered: 1. What if there\u2019s noise in the",
            "more classes2. While some learning algorithms naturally allow for more than two classes, others are by nature binary classi\ufb01cation algorithms. There are strategies allowing to turn a binary classi\ufb01cation learning algorithm into a multiclass one. I talk about one of them in Chapter 7. Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. 1Multiplication of many numbers can give either a very small result or a very large one. It often results in the problem of numerical over\ufb02ow when the machine cannot store such extreme numbers in memory. 2There\u2019s still one label per example though. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 12 The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target. 2.7 Model-Based vs. Instance-Based Learning Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were w\u0153 and b\u0153. After the model was built, the training data can be discarded. Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classi\ufb01cation, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood. 2.8 Shallow vs. Deep Learning A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speci\ufb01cally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers. Don\u2019t worry if you don\u2019t understand what that means right now. We look at neural networks more closely in Chapter 6. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 13 The Hundred- Page Machine Learning Book Andriy Burkov \u201cAll models are wrong, but some are useful.\u201d \u2014 George Box The book is distributed on the \u201cread \ufb01rst, buy later\u201d principle. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Fundamental Algorithms In this chapter, I describe \ufb01ve algorithms which are not just the most known but also either very e\ufb00ective on their own or are used as building blocks for the most e\ufb00ective learning algorithms out there. 3.1 Linear Regression Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. 3.1.1 Problem Statement We have a collection of labeled examples {(xi, yi)}N i=1, where N is the size of the collection, xi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target and every feature x(j) i , j = 1, . . . , D, is also a real number. We want to build a model fw,b(x) as a linear combination of features of example x: fw,b(x) = wx + b, (1) where w is a D-dimensional vector of parameters and b is a real number. The notation fw,b means that the model f is parametrized by two values: w and b. We will use the model to predict the unknown y for a given x like this: y \u03a9 fw,b(x). Two models parametrized by two di\ufb00erent pairs (w, b) will likely produce two di\ufb00erent predictions when applied to the same example. We want to \ufb01nd the optimal values (w\u00fa, b\u00fa). Obviously, the optimal values of parameters de\ufb01ne the model that makes the most accurate predictions. You could have noticed that the form of our linear model in eq. 1 is very similar to the form of the SVM model. The only di\ufb00erence is the missing sign operator. The two models are indeed similar. However, the hyperplane in the SVM plays the role of the decision boundary: it\u2019s used to separate two groups of examples from one another. As such, it has to be as far from each group as possible. On the other hand, the hyperplane in linear regression is chosen to be as close to all training examples as possible. You can see why this latter requirement is essential by looking at the illustration in \ufb01g. 1. It displays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We can use this line to predict the value of the target ynew for a new unlabeled input example xnew. If our examples are D-dimensional feature vectors (for D > 1), the only di\ufb00erence 1To say that yi is real-valued, we write yi \u0153 R, where R denotes the set of all real numbers, an in\ufb01nite set of numbers from minus in\ufb01nity to plus in\ufb01nity. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3 Figure 1: Linear Regression for one-dimensional examples. with the one-dimensional case is that the regression model is not a line but a plane (for two dimensions) or a hyperplane (for D > 2). Now you see why it\u2019s essential to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in \ufb01g. 1 was far from the blue dots, the prediction ynew would have fewer chances to be correct. 3.1.2 Solution To get this latter requirement satis\ufb01ed, the optimization procedure which we use to \ufb01nd the optimal values for w\u00fa and b\u00fa tries to minimize the following expression: 1 N \u00ff i=1...N (fw,b(xi) \u2260yi)2. (2) In mathematics, the expression we minimize or maximize is called an objective function, or, simply, an objective. The expression (f(xi) \u2260yi)2 in the above objective is called the loss function. It\u2019s a measure of penalty for misclassi\ufb01cation of example i. This particular choice of the loss function is called squared error loss. All model-based learning algorithms have a loss function and what we do to \ufb01nd the best model is we try to minimize the objective known as the cost function. In linear regression, the cost function is given by the average loss, also called the empirical risk. The average loss, or empirical risk, for a model, is the average of all penalties obtained by applying the model to the training data. Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4 Why is the loss in linear regression a quadratic function? Why couldn\u2019t we get the absolute value of the di\ufb00erence between the true target yi and the predicted value f(xi) and use that as a penalty? We could. Moreover, we also could use a cube instead of a square. Now you probably start realizing how many seemingly arbitrary decisions are made when we design a machine learning algorithm: we decided to use the linear combination of features to predict the target. However, we could use a square or some other polynomial to combine the values of features. We could also use some other loss function that makes sense: the absolute di\ufb00erence between f(xi) and yi makes sense, the cube of the di\ufb00erence too; the binary loss (1 when f(xi) and yi are di\ufb00erent and 0 when they are the same) also makes sense, right? If we made di\ufb00erent decisions about the form of the model, the form of the loss function, and about the choice of the algorithm that minimizes the average loss to \ufb01nd the best values of parameters, we would end up inventing a di\ufb00erent machine learning algorithm. Sounds easy, doesn\u2019t it? However, do not rush to invent a new learning algorithm. The fact that it\u2019s di\ufb00erent doesn\u2019t mean that it will work better in practice. People invent new learning algorithms for one of the two main reasons: 1. The new algorithm solves a speci\ufb01c practical problem better than the existing algorithms. 2. The new algorithm has better theoretical guarantees on the quality of the model it produces. One practical justi\ufb01cation of the choice of the linear form for the model is that it\u2019s simple. Why use a complex model when you can use a simple one? Another consideration is that linear models rarely over\ufb01t. Over\ufb01tting is the property of a model such that the model predicts very well labels of the examples used during training but frequently makes errors when applied to examples that weren\u2019t seen by the learning algorithm during training. An example of over\ufb01tting in regression is shown in \ufb01g. 2. The data used to build the red regression line is the same as in \ufb01g. 1. The di\ufb00erence is that this time, this is the polynomial regression with a polynomial of degree 10. The regression line predicts almost perfectly the targets almost all training examples, but will likely make signi\ufb01cant errors on new data, as you can see in \ufb01g. 1 for xnew. We talk more about over\ufb01tting and how to avoid it Chapter 5. Now you know why linear regression can be useful: it doesn\u2019t over\ufb01t much. But what about the squared loss? Why did we decide that it should be squared? In 1705, the French mathematician Adrien-Marie Legendre, who \ufb01rst published the sum of squares method for gauging the quality of the model stated that squaring the error before summing is convenient. Why did he say that? The absolute value is not convenient, because it doesn\u2019t have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary di\ufb03culties when employing linear algebra to \ufb01nd closed form solutions to optimization problems. Closed form solutions to \ufb01nding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). Intuitively, squared penalties are also advantageous because they exaggerate the di\ufb00erence between the true target and the predicted one according to the value of this di\ufb00erence. We Andriy Burkov The Hundred-Page Machine Learning Book - Draft 5 y x new new Figure 2: Over\ufb01tting. might also use the powers 3 or 4, but their derivatives are more complicated to work with. Finally, why do we care about the derivative of the average loss? Remember from algebra that if we can calculate the gradient of the function in eq. 2, we can then set this gradient to zero2 and \ufb01nd the solution to a system of equations that gives us the optimal values w\u00fa and b\u00fa. You can spend several minutes and check it yourself. 3.2 Logistic Regression The \ufb01rst thing to say is that logistic regression is not a regression, but a classi\ufb01cation learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. I explain logistic regression on the case of binary classi\ufb01cation. However, it can naturally be extended to multiclass classi\ufb01cation. 3.2.1 Problem Statement In logistic regression, we still want to model yi as a linear function of xi, however, with a binary yi this is not straightforward. The linear combination of features such as wxi + b is a function that spans from minus in\ufb01nity to plus in\ufb01nity, while yi has only two possible values. 2To \ufb01nd the minimum or the maximum of a function, we set the gradient to zero because the value of the gradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line. Andriy Burkov The Hundred-Page Machine Learning"
        ]
    }
]