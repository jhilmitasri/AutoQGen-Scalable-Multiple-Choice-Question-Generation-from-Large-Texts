{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb843d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and API Key Configuration\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# For Google Colab:\n",
    "# from google.colab import userdata\n",
    "# client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "\n",
    "# For local environment (make sure you have OPENAI_API_KEY set):\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# As a placeholder, initialize the client.\n",
    "# Make sure to replace this with your actual setup.\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    print(\"OpenAI client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    print(\"Please ensure your API key is configured correctly.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update this path to point to the PDF you want to process\n",
    "PDF_FILE_PATH = \"Documents/The 100 Page Machine Learning Book Part2.pdf\"\n",
    "OUTPUT_JSON_PATH = \"quiz_output.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c12f3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Document Processing and Parsing Functions\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text content from a PDF file.\"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "        print(f\"✅ Successfully extracted {len(full_text.split())} words from {pdf_path}.\")\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading PDF {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 200) -> list[str]:\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunks.append(\" \".join(words[i:i + chunk_size]))\n",
    "    print(f\"Text split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def parse_json_from_response(response_text: str) -> dict | None:\n",
    "    \"\"\"Safely extracts a JSON object from a string, even with surrounding text.\"\"\"\n",
    "    # Use regex to find the JSON block, which handles leading/trailing text\n",
    "    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    if not json_match:\n",
    "        print(\"Parser Error: No JSON object found in the response.\")\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(json_match.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Parser Error: Failed to decode JSON from the extracted string.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b44e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Core LLM Functions\n",
    "def extract_concepts_from_chunk(chunk: str, client) -> list[str]:\n",
    "    \"\"\"Uses gpt-3.5-turbo to extract key concepts from a text chunk.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following text excerpt, please extract the most critical main ideas and concepts.\n",
    "    Respond with a simple bulleted list.\n",
    "\n",
    "    Excerpt:\n",
    "    \\\"\\\"\\\"\n",
    "    {chunk}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    Concepts:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return [line.strip('- ') for line in content.strip().split('\\n') if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"API Error during concept extraction: {e}\")\n",
    "        return []\n",
    "\n",
    "def synthesize_concepts(all_concepts: list[str], client) -> list[str]:\n",
    "    \"\"\"Merges and deduplicates a list of concepts using gpt-3.5-turbo.\"\"\"\n",
    "    concepts_str = \"\\n\".join([f\"- {c}\" for c in all_concepts])\n",
    "    prompt = f\"\"\"\n",
    "    I have a list of concepts from a document. Merge and deduplicate this into a final, clean list.\n",
    "\n",
    "    Extracted Concepts:\n",
    "    \\\"\\\"\\\"\n",
    "    {concepts_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    Consolidated Concepts (bulleted list):\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return [line.strip('- ') for line in content.strip().split('\\n') if line.strip()]\n",
    "\n",
    "def retrieve_relevant_passages(concept: str, text_chunks: list[str], top_k: int = 2) -> list[str]:\n",
    "    \"\"\"Retrieves the most relevant text chunks for a given concept using keyword matching.\"\"\"\n",
    "    concept_words = set(concept.lower().split())\n",
    "    scored_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        score = len(concept_words.intersection(chunk_words))\n",
    "        if score > 0:\n",
    "            scored_chunks.append((score, chunk))\n",
    "\n",
    "    scored_chunks.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [chunk for score, chunk in scored_chunks[:top_k]]\n",
    "\n",
    "def generate_question_with_difficulty(concept: str, passages: list[str], difficulty: str, client) -> dict | None:\n",
    "    \"\"\"Generates a question with a specific difficulty level (CORRECTED VERSION).\"\"\"\n",
    "\n",
    "    difficulty_instructions = {\n",
    "        \"easy\": \"The question should test basic recall or understanding of a key definition from the text (Bloom's Taxonomy: Remembering/Understanding).\",\n",
    "        \"medium\": \"The question should require applying a concept to a new context or analyzing the relationship between ideas from the text (Bloom's Taxonomy: Applying/Analyzing).\",\n",
    "        \"hard\": \"The question should require evaluating the strengths/weaknesses of an argument or synthesizing information from multiple passages to form a conclusion (Bloom's Taxonomy: Evaluating/Creating).\"\n",
    "    }\n",
    "    \n",
    "    instruction = difficulty_instructions.get(difficulty, difficulty_instructions['medium'])\n",
    "    passages_str = \"\\\\n\\\\n---\\\\n\\\\n\".join(passages)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on the following main idea and relevant text, create one multiple-choice question.\n",
    "\n",
    "    **Difficulty Instruction**: {instruction}\n",
    "    **Main Idea**: \"{concept}\"\n",
    "    **Relevant Text**:\n",
    "    \\\"\\\"\\\"\n",
    "    {passages_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    Your response MUST be ONLY a single JSON object with the exact keys: \"question\", \"choices\", \"correct_answer\", \"explanation\".\n",
    "    The incorrect choices must be plausible distractors.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Make the API call\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # 2. DEFINE response_content from the result\n",
    "        response_content = response.choices[0].message.content\n",
    "        \n",
    "        # 3. NOW use response_content to parse the JSON\n",
    "        parsed_json = parse_json_from_response(response_content)\n",
    "        \n",
    "        if parsed_json:\n",
    "            parsed_json['difficulty'] = difficulty\n",
    "        \n",
    "        return parsed_json\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed for concept '{concept}': {e}\")\n",
    "        return None\n",
    "\n",
    "def score_question_quality(question_data: dict, client) -> int:\n",
    "    \"\"\"Uses an AI judge to score the quality of a generated question (1-5).\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please evaluate the quality of the following multiple-choice question on a scale of 1 to 5,\n",
    "    where 1 is poor and 5 is excellent. Consider its clarity, conceptual depth, and the plausibility of its distractors.\n",
    "\n",
    "    Question: {question_data['question']}\n",
    "    Choices: {question_data['choices']}\n",
    "\n",
    "    Return ONLY a single integer score between 1 and 5.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", # The cheap model is fine for this simple task\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return int(response.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Quality scoring failed: {e}\")\n",
    "        return 0 # Return a default low score on failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01ebe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully extracted 6490 words from Documents/The 100 Page Machine Learning Book Part2.pdf.\n",
      "Text split into 4 chunks.\n",
      "\n",
      "--- Starting Stage 2: Concept Extraction ---\n",
      "Extracting concepts from chunk 1/4...\n",
      "Extracting concepts from chunk 2/4...\n",
      "Extracting concepts from chunk 3/4...\n",
      "Extracting concepts from chunk 4/4...\n",
      "\n",
      "Extracted 32 raw concepts. Now synthesizing...\n",
      "Synthesized down to 27 final concepts.\n",
      "\n",
      "--- Starting Stage 3: Question Generation ---\n",
      "Processing concept 1/27 (Difficulty: easy): 'Derivative and Gradient...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 2/27 (Difficulty: medium): 'Random Variable...'\n",
      "  -> ✅ Kept question with quality score: 5\n",
      "Processing concept 3/27 (Difficulty: hard): 'Probability Mass Function and Probability Density ...'\n",
      "  -> ✅ Kept question with quality score: 5\n",
      "Processing concept 4/27 (Difficulty: easy): 'Unbiased Estimators...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 5/27 (Difficulty: medium): 'Bayes' Rule...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 6/27 (Difficulty: hard): 'Parameter Estimation...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 7/27 (Difficulty: easy): 'Classiﬁcation vs. Regression...'\n",
      "Parser Error: Failed to decode JSON from the extracted string.\n",
      "  -> Failed to generate question.\n",
      "Processing concept 8/27 (Difficulty: medium): 'Model-Based vs. Instance-Based Learning...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 9/27 (Difficulty: hard): 'Binary classification vs. multiclass classificatio...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 10/27 (Difficulty: easy): 'Regression as predicting real-valued labels...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 11/27 (Difficulty: medium): 'Shallow learning vs. deep learning...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 12/27 (Difficulty: hard): 'Linear regression as a popular regression learning...'\n",
      "  -> ✅ Kept question with quality score: 5\n",
      "Processing concept 13/27 (Difficulty: easy): 'Optimization in linear regression using squared er...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 14/27 (Difficulty: medium): 'Overfitting in regression models...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 15/27 (Difficulty: hard): 'Logistic regression as a classification learning a...'\n",
      "  -> ✅ Kept question with quality score: 5\n",
      "Processing concept 16/27 (Difficulty: easy): 'Gradient descent for optimization in machine learn...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 17/27 (Difficulty: medium): 'Decision tree learning involves building a decisio...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 18/27 (Difficulty: hard): 'ID3 algorithm optimizes the average log-likelihood...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 19/27 (Difficulty: easy): 'Entropy is used in ID3 to evaluate the goodness of...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 20/27 (Difficulty: medium): 'Support Vector Machine (SVM) is briefly mentioned ...'\n",
      "  -> ✅ Kept question with quality score: 5\n",
      "Processing concept 21/27 (Difficulty: hard): 'Decision tree depth and splitting criteria in ID3 ...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 22/27 (Difficulty: easy): 'Entropy-based split criterion for decision trees...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 23/27 (Difficulty: medium): 'Dealing with noise in data using hinge loss functi...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 24/27 (Difficulty: hard): 'Trade-off parameter C in SVM for balancing margin ...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 25/27 (Difficulty: easy): 'Kernel trick for transforming data into higher-dim...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 26/27 (Difficulty: medium): 'Use of kernel functions in SVM optimization...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "Processing concept 27/27 (Difficulty: hard): 'Optimization algorithm for SVM using Lagrange mult...'\n",
      "  -> ✅ Kept question with quality score: 4\n",
      "\n",
      "✅ Pipeline complete! Generated 26 questions.\n",
      "Output saved to quiz_output.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run the Full Pipeline\n",
    "def run_full_pipeline(pdf_path, client, desired_difficulties=[\"easy\", \"medium\", \"hard\"]):\n",
    "    # Stage 1: Ingest and Chunk Document\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    if not document_text:\n",
    "        print(\"Pipeline stopped: Could not read document.\")\n",
    "        return []\n",
    "    text_chunks = chunk_text(document_text)\n",
    "\n",
    "    # Stage 2: Extract and Synthesize Concepts\n",
    "    print(\"\\n--- Starting Stage 2: Concept Extraction ---\")\n",
    "    all_chunk_concepts = []\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        print(f\"Extracting concepts from chunk {i+1}/{len(text_chunks)}...\")\n",
    "        all_chunk_concepts.extend(extract_concepts_from_chunk(chunk, client))\n",
    "\n",
    "    print(f\"\\nExtracted {len(all_chunk_concepts)} raw concepts. Now synthesizing...\")\n",
    "    final_concepts = synthesize_concepts(all_chunk_concepts, client)\n",
    "    print(f\"Synthesized down to {len(final_concepts)} final concepts.\")\n",
    "\n",
    "    # Stage 3: Retrieve and Generate Questions\n",
    "    print(\"\\n--- Starting Stage 3: Question Generation ---\")\n",
    "    generated_questions = []\n",
    "    \n",
    "    for i, concept in enumerate(final_concepts):\n",
    "        # ✅ CHANGE 2: Determine the difficulty for the current question\n",
    "        current_difficulty = desired_difficulties[i % len(desired_difficulties)]\n",
    "        \n",
    "        print(f\"Processing concept {i+1}/{len(final_concepts)} (Difficulty: {current_difficulty}): '{concept[:50]}...'\")\n",
    "        passages = retrieve_relevant_passages(concept, text_chunks)\n",
    "        \n",
    "        if passages:\n",
    "            # ✅ CHANGE 3: Call the new function with the difficulty parameter\n",
    "            question_data = generate_question_with_difficulty(concept, passages, current_difficulty, client)\n",
    "\n",
    "            if question_data:\n",
    "                # Add the quality score\n",
    "                quality_score = score_question_quality(question_data, client)\n",
    "                question_data['quality_score'] = quality_score\n",
    "                \n",
    "                # Only append the question if it meets our quality threshold\n",
    "                if quality_score >= 3: # Example threshold\n",
    "                    generated_questions.append(question_data)\n",
    "                    print(f\"  -> ✅ Kept question with quality score: {quality_score}\")\n",
    "                else:\n",
    "                    print(f\"  -> ❌ Discarded question with low quality score: {quality_score}\")\n",
    "            else:\n",
    "                print(\"  -> Failed to generate question.\")\n",
    "        else:\n",
    "            print(\"  -> No relevant passages found, skipping.\")\n",
    "\n",
    "    return generated_questions\n",
    "\n",
    "# --- EXECUTE ---\n",
    "if os.path.exists(PDF_FILE_PATH):\n",
    "    final_quiz = run_full_pipeline(PDF_FILE_PATH, client)\n",
    "\n",
    "    # Save the final list of questions to a JSON file\n",
    "    with open(OUTPUT_JSON_PATH, 'w') as f:\n",
    "        json.dump(final_quiz, f, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ Pipeline complete! Generated {len(final_quiz)} questions.\")\n",
    "    print(f\"Output saved to {OUTPUT_JSON_PATH}\")\n",
    "else:\n",
    "    print(f\"❌ Error: The file '{PDF_FILE_PATH}' was not found. Please update the path in Cell 2.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
